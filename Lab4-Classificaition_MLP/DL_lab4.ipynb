{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxEOqOnbXW3T",
        "outputId": "caa9dad0-2233-456c-e218-28826f0e2303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPsQS-fMbn9m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spam Email Classification"
      ],
      "metadata": {
        "id": "wpw1H3iUS8Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/email.csv\")\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "uddhfqiodsrC",
        "outputId": "3a3fdc9c-daab-46b2-a233-945e68db7fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Category                                            Message\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "5     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
              "6      ham  Even my brother is not like to speak with me. ...\n",
              "7      ham  As per your request 'Melle Melle (Oru Minnamin...\n",
              "8     spam  WINNER!! As a valued network customer you have...\n",
              "9     spam  Had your mobile 11 months or more? U R entitle..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-393d0b9e-af58-4a86-8db6-8ffd2ed757d0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>spam</td>\n",
              "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ham</td>\n",
              "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>spam</td>\n",
              "      <td>WINNER!! As a valued network customer you have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>spam</td>\n",
              "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-393d0b9e-af58-4a86-8db6-8ffd2ed757d0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-393d0b9e-af58-4a86-8db6-8ffd2ed757d0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-393d0b9e-af58-4a86-8db6-8ffd2ed757d0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8ed95344-88a5-43e4-a021-8df7a7c666c9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8ed95344-88a5-43e4-a021-8df7a7c666c9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8ed95344-88a5-43e4-a021-8df7a7c666c9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5573,\n  \"fields\": [\n    {\n      \"column\": \"Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"ham\",\n          \"spam\",\n          \"{\\\"mode\\\":\\\"full\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Message\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5158,\n        \"samples\": [\n          \"&lt;#&gt;  am I think? Should say on syllabus\",\n          \"Yar lor... How u noe? U used dat route too?\",\n          \"En chikku nange bakra msg kalstiya..then had tea/coffee?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(message):\n",
        "    message_lower = message.lower()\n",
        "\n",
        "    # Keywords\n",
        "    keywords = [\n",
        "        \"win\", \"winner\", \"winning\", \"lottery\", \"jackpot\", \"prize\", \"reward\",\n",
        "        \"cash\", \"free\", \"offer\", \"bonus\", \"deal\", \"cheap\", \"bargain\", \"discount\", \"sale\", \"urgent\", \"limited\", \"exclusive\", \"only\", \"now\", \"instant\", \"act now\", \"click\", \"subscribe\", \"buy\", \"order\", \"trial\", \"guarantee\", \"credit\", \"loan\", \"debt\", \"money\", \"rich\", \"income\", \"investment\", \"viagra\", \"pharmacy\", \"pills\", \"medicine\", \"unsubscribe\",\"congratulations\", \"claim\", \"gift\", \"winner\"\n",
        "    ]\n",
        "    keyword_present = int(any(word in message_lower for word in keywords))\n",
        "\n",
        "    # Check link\n",
        "    contains_link = int(\"http\" in message_lower or \"www\" in message_lower)\n",
        "\n",
        "    # Contains attachment\n",
        "    attachment_keywords = [\".pdf\", \".doc\", \".xls\", \"attachment\"]\n",
        "    contains_attachment = int(any(word in message_lower for word in attachment_keywords))\n",
        "\n",
        "    # Use of capital letters (ratio)\n",
        "    letters = [c for c in message if c.isalpha()]\n",
        "    if letters:\n",
        "        caps_ratio = sum(1 for c in letters if c.isupper()) / len(letters)\n",
        "    else:\n",
        "        caps_ratio = 0.0\n",
        "    high_caps = int(caps_ratio > 0.3)\n",
        "\n",
        "    return [keyword_present, contains_link, contains_attachment, high_caps]\n",
        "\n",
        "# Get Message and preprocess\n",
        "features = df[\"Message\"].apply(extract_features)\n",
        "X = np.array(features.tolist())\n",
        "\n",
        "# spam=1, ham=0\n",
        "y = df[\"Category\"].map({\"ham\": 0, \"spam\": 1}).values\n",
        "\n",
        "preprocessed_df = pd.DataFrame(X, columns=[\"Keyword\", \"Link\", \"Attachment\", \"HighCaps\"])\n",
        "preprocessed_df[\"Label\"] = y\n",
        "\n",
        "preprocessed_df.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "uRv4XGz4axI3",
        "outputId": "fa0b7190-ac48-4d55-c277-2a68ccf78669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Keyword  Link  Attachment  HighCaps  Label\n",
              "0         1     0           0         0    0.0\n",
              "1         0     0           0         0    0.0\n",
              "2         1     0           0         0    1.0\n",
              "3         0     0           0         0    0.0\n",
              "4         0     0           0         0    0.0\n",
              "5         1     0           0         0    1.0\n",
              "6         0     0           0         0    0.0\n",
              "7         0     0           0         0    0.0\n",
              "8         1     0           0         0    1.0\n",
              "9         1     0           0         0    1.0\n",
              "10        0     0           0         0    0.0\n",
              "11        1     0           0         0    1.0\n",
              "12        1     1           0         1    1.0\n",
              "13        0     0           0         0    0.0\n",
              "14        0     0           0         1    0.0\n",
              "15        1     1           0         0    1.0\n",
              "16        0     0           0         0    0.0\n",
              "17        0     0           0         0    0.0\n",
              "18        0     0           0         0    0.0\n",
              "19        0     0           0         1    1.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb136764-4d20-4bfc-b7be-3428fdcb2ef6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Keyword</th>\n",
              "      <th>Link</th>\n",
              "      <th>Attachment</th>\n",
              "      <th>HighCaps</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb136764-4d20-4bfc-b7be-3428fdcb2ef6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bb136764-4d20-4bfc-b7be-3428fdcb2ef6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bb136764-4d20-4bfc-b7be-3428fdcb2ef6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bc4102a0-e5e6-46a3-9352-e06ab16b51cc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bc4102a0-e5e6-46a3-9352-e06ab16b51cc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bc4102a0-e5e6-46a3-9352-e06ab16b51cc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "preprocessed_df",
              "summary": "{\n  \"name\": \"preprocessed_df\",\n  \"rows\": 5573,\n  \"fields\": [\n    {\n      \"column\": \"Keyword\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Link\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Attachment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HighCaps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3407507548977646,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/drive/MyDrive/Colab Notebooks/preprocessed_email.csv\"\n",
        "preprocessed_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"File saved to: {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NHgyV_scqKb",
        "outputId": "0fd9d9f4-bb72-40bb-f1f8-f696e043bb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to: /content/drive/MyDrive/Colab Notebooks/preprocessed_email.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/preprocessed_email.csv\")\n",
        "\n",
        "data[\"Label\"] = pd.to_numeric(data[\"Label\"], errors=\"coerce\")\n",
        "data = data.dropna(subset=[\"Label\"])\n",
        "data[\"Label\"] = data[\"Label\"].astype(int)\n",
        "\n",
        "X = data[[\"Keyword\", \"Link\", \"Attachment\", \"HighCaps\"]].values\n",
        "y = data[\"Label\"].values.reshape(-1, 1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Helper functions ---\n",
        "def sigmoid(z):\n",
        "    z = np.clip(z, -500, 500)  # prevent overflow\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(a):\n",
        "    return a * (1 - a)\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "limit1 = np.sqrt(6 / (input_size + hidden_size))\n",
        "W1 = np.random.uniform(-limit1, limit1, (input_size, hidden_size))\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "\n",
        "limit2 = np.sqrt(6 / (hidden_size + output_size))\n",
        "W2 = np.random.uniform(-limit2, limit2, (hidden_size, output_size))\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # Forward\n",
        "    Z1 = np.dot(X_train, W1) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    # MSE Loss\n",
        "    loss = np.mean((y_train - A2) ** 2)\n",
        "\n",
        "    # Backprop\n",
        "    m = X_train.shape[0]\n",
        "    dZ2 = (A2 - y_train) * sigmoid_derivative(A2)\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * sigmoid_derivative(A1)\n",
        "    dW1 = np.dot(X_train.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Update\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss:.6f}\")\n",
        "\n",
        "# --- Prediction ---\n",
        "def predict(X):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    return (A2 >= 0.5).astype(int)\n",
        "\n",
        "# --- Evaluate ---\n",
        "y_pred = predict(X_test)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLd1o3ICc2Ko",
        "outputId": "0afefad9-560b-457e-c8ea-7b8ad1eda35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.161395\n",
            "Epoch 2, Loss: 0.161285\n",
            "Epoch 3, Loss: 0.161175\n",
            "Epoch 4, Loss: 0.161065\n",
            "Epoch 5, Loss: 0.160955\n",
            "Epoch 6, Loss: 0.160846\n",
            "Epoch 7, Loss: 0.160737\n",
            "Epoch 8, Loss: 0.160629\n",
            "Epoch 9, Loss: 0.160521\n",
            "Epoch 10, Loss: 0.160413\n",
            "Epoch 11, Loss: 0.160305\n",
            "Epoch 12, Loss: 0.160198\n",
            "Epoch 13, Loss: 0.160091\n",
            "Epoch 14, Loss: 0.159984\n",
            "Epoch 15, Loss: 0.159878\n",
            "Epoch 16, Loss: 0.159772\n",
            "Epoch 17, Loss: 0.159666\n",
            "Epoch 18, Loss: 0.159561\n",
            "Epoch 19, Loss: 0.159456\n",
            "Epoch 20, Loss: 0.159351\n",
            "Epoch 21, Loss: 0.159246\n",
            "Epoch 22, Loss: 0.159142\n",
            "Epoch 23, Loss: 0.159038\n",
            "Epoch 24, Loss: 0.158935\n",
            "Epoch 25, Loss: 0.158831\n",
            "Epoch 26, Loss: 0.158728\n",
            "Epoch 27, Loss: 0.158625\n",
            "Epoch 28, Loss: 0.158523\n",
            "Epoch 29, Loss: 0.158421\n",
            "Epoch 30, Loss: 0.158319\n",
            "Epoch 31, Loss: 0.158218\n",
            "Epoch 32, Loss: 0.158116\n",
            "Epoch 33, Loss: 0.158015\n",
            "Epoch 34, Loss: 0.157915\n",
            "Epoch 35, Loss: 0.157814\n",
            "Epoch 36, Loss: 0.157714\n",
            "Epoch 37, Loss: 0.157614\n",
            "Epoch 38, Loss: 0.157515\n",
            "Epoch 39, Loss: 0.157416\n",
            "Epoch 40, Loss: 0.157317\n",
            "Epoch 41, Loss: 0.157218\n",
            "Epoch 42, Loss: 0.157119\n",
            "Epoch 43, Loss: 0.157021\n",
            "Epoch 44, Loss: 0.156924\n",
            "Epoch 45, Loss: 0.156826\n",
            "Epoch 46, Loss: 0.156729\n",
            "Epoch 47, Loss: 0.156632\n",
            "Epoch 48, Loss: 0.156535\n",
            "Epoch 49, Loss: 0.156439\n",
            "Epoch 50, Loss: 0.156342\n",
            "Epoch 51, Loss: 0.156246\n",
            "Epoch 52, Loss: 0.156151\n",
            "Epoch 53, Loss: 0.156055\n",
            "Epoch 54, Loss: 0.155960\n",
            "Epoch 55, Loss: 0.155866\n",
            "Epoch 56, Loss: 0.155771\n",
            "Epoch 57, Loss: 0.155677\n",
            "Epoch 58, Loss: 0.155583\n",
            "Epoch 59, Loss: 0.155489\n",
            "Epoch 60, Loss: 0.155395\n",
            "Epoch 61, Loss: 0.155302\n",
            "Epoch 62, Loss: 0.155209\n",
            "Epoch 63, Loss: 0.155117\n",
            "Epoch 64, Loss: 0.155024\n",
            "Epoch 65, Loss: 0.154932\n",
            "Epoch 66, Loss: 0.154840\n",
            "Epoch 67, Loss: 0.154749\n",
            "Epoch 68, Loss: 0.154657\n",
            "Epoch 69, Loss: 0.154566\n",
            "Epoch 70, Loss: 0.154475\n",
            "Epoch 71, Loss: 0.154385\n",
            "Epoch 72, Loss: 0.154294\n",
            "Epoch 73, Loss: 0.154204\n",
            "Epoch 74, Loss: 0.154114\n",
            "Epoch 75, Loss: 0.154025\n",
            "Epoch 76, Loss: 0.153935\n",
            "Epoch 77, Loss: 0.153846\n",
            "Epoch 78, Loss: 0.153757\n",
            "Epoch 79, Loss: 0.153669\n",
            "Epoch 80, Loss: 0.153580\n",
            "Epoch 81, Loss: 0.153492\n",
            "Epoch 82, Loss: 0.153404\n",
            "Epoch 83, Loss: 0.153317\n",
            "Epoch 84, Loss: 0.153230\n",
            "Epoch 85, Loss: 0.153142\n",
            "Epoch 86, Loss: 0.153056\n",
            "Epoch 87, Loss: 0.152969\n",
            "Epoch 88, Loss: 0.152883\n",
            "Epoch 89, Loss: 0.152796\n",
            "Epoch 90, Loss: 0.152710\n",
            "Epoch 91, Loss: 0.152625\n",
            "Epoch 92, Loss: 0.152539\n",
            "Epoch 93, Loss: 0.152454\n",
            "Epoch 94, Loss: 0.152369\n",
            "Epoch 95, Loss: 0.152285\n",
            "Epoch 96, Loss: 0.152200\n",
            "Epoch 97, Loss: 0.152116\n",
            "Epoch 98, Loss: 0.152032\n",
            "Epoch 99, Loss: 0.151948\n",
            "Epoch 100, Loss: 0.151865\n",
            "Epoch 101, Loss: 0.151781\n",
            "Epoch 102, Loss: 0.151698\n",
            "Epoch 103, Loss: 0.151615\n",
            "Epoch 104, Loss: 0.151533\n",
            "Epoch 105, Loss: 0.151450\n",
            "Epoch 106, Loss: 0.151368\n",
            "Epoch 107, Loss: 0.151286\n",
            "Epoch 108, Loss: 0.151205\n",
            "Epoch 109, Loss: 0.151123\n",
            "Epoch 110, Loss: 0.151042\n",
            "Epoch 111, Loss: 0.150961\n",
            "Epoch 112, Loss: 0.150880\n",
            "Epoch 113, Loss: 0.150799\n",
            "Epoch 114, Loss: 0.150719\n",
            "Epoch 115, Loss: 0.150639\n",
            "Epoch 116, Loss: 0.150559\n",
            "Epoch 117, Loss: 0.150479\n",
            "Epoch 118, Loss: 0.150400\n",
            "Epoch 119, Loss: 0.150321\n",
            "Epoch 120, Loss: 0.150242\n",
            "Epoch 121, Loss: 0.150163\n",
            "Epoch 122, Loss: 0.150084\n",
            "Epoch 123, Loss: 0.150006\n",
            "Epoch 124, Loss: 0.149928\n",
            "Epoch 125, Loss: 0.149850\n",
            "Epoch 126, Loss: 0.149772\n",
            "Epoch 127, Loss: 0.149694\n",
            "Epoch 128, Loss: 0.149617\n",
            "Epoch 129, Loss: 0.149540\n",
            "Epoch 130, Loss: 0.149463\n",
            "Epoch 131, Loss: 0.149386\n",
            "Epoch 132, Loss: 0.149310\n",
            "Epoch 133, Loss: 0.149234\n",
            "Epoch 134, Loss: 0.149158\n",
            "Epoch 135, Loss: 0.149082\n",
            "Epoch 136, Loss: 0.149006\n",
            "Epoch 137, Loss: 0.148931\n",
            "Epoch 138, Loss: 0.148856\n",
            "Epoch 139, Loss: 0.148781\n",
            "Epoch 140, Loss: 0.148706\n",
            "Epoch 141, Loss: 0.148631\n",
            "Epoch 142, Loss: 0.148557\n",
            "Epoch 143, Loss: 0.148482\n",
            "Epoch 144, Loss: 0.148408\n",
            "Epoch 145, Loss: 0.148335\n",
            "Epoch 146, Loss: 0.148261\n",
            "Epoch 147, Loss: 0.148188\n",
            "Epoch 148, Loss: 0.148114\n",
            "Epoch 149, Loss: 0.148041\n",
            "Epoch 150, Loss: 0.147969\n",
            "Epoch 151, Loss: 0.147896\n",
            "Epoch 152, Loss: 0.147824\n",
            "Epoch 153, Loss: 0.147751\n",
            "Epoch 154, Loss: 0.147679\n",
            "Epoch 155, Loss: 0.147608\n",
            "Epoch 156, Loss: 0.147536\n",
            "Epoch 157, Loss: 0.147464\n",
            "Epoch 158, Loss: 0.147393\n",
            "Epoch 159, Loss: 0.147322\n",
            "Epoch 160, Loss: 0.147251\n",
            "Epoch 161, Loss: 0.147181\n",
            "Epoch 162, Loss: 0.147110\n",
            "Epoch 163, Loss: 0.147040\n",
            "Epoch 164, Loss: 0.146970\n",
            "Epoch 165, Loss: 0.146900\n",
            "Epoch 166, Loss: 0.146830\n",
            "Epoch 167, Loss: 0.146761\n",
            "Epoch 168, Loss: 0.146691\n",
            "Epoch 169, Loss: 0.146622\n",
            "Epoch 170, Loss: 0.146553\n",
            "Epoch 171, Loss: 0.146484\n",
            "Epoch 172, Loss: 0.146416\n",
            "Epoch 173, Loss: 0.146347\n",
            "Epoch 174, Loss: 0.146279\n",
            "Epoch 175, Loss: 0.146211\n",
            "Epoch 176, Loss: 0.146143\n",
            "Epoch 177, Loss: 0.146075\n",
            "Epoch 178, Loss: 0.146008\n",
            "Epoch 179, Loss: 0.145941\n",
            "Epoch 180, Loss: 0.145873\n",
            "Epoch 181, Loss: 0.145806\n",
            "Epoch 182, Loss: 0.145740\n",
            "Epoch 183, Loss: 0.145673\n",
            "Epoch 184, Loss: 0.145607\n",
            "Epoch 185, Loss: 0.145540\n",
            "Epoch 186, Loss: 0.145474\n",
            "Epoch 187, Loss: 0.145408\n",
            "Epoch 188, Loss: 0.145343\n",
            "Epoch 189, Loss: 0.145277\n",
            "Epoch 190, Loss: 0.145212\n",
            "Epoch 191, Loss: 0.145146\n",
            "Epoch 192, Loss: 0.145081\n",
            "Epoch 193, Loss: 0.145017\n",
            "Epoch 194, Loss: 0.144952\n",
            "Epoch 195, Loss: 0.144887\n",
            "Epoch 196, Loss: 0.144823\n",
            "Epoch 197, Loss: 0.144759\n",
            "Epoch 198, Loss: 0.144695\n",
            "Epoch 199, Loss: 0.144631\n",
            "Epoch 200, Loss: 0.144567\n",
            "Epoch 201, Loss: 0.144504\n",
            "Epoch 202, Loss: 0.144440\n",
            "Epoch 203, Loss: 0.144377\n",
            "Epoch 204, Loss: 0.144314\n",
            "Epoch 205, Loss: 0.144251\n",
            "Epoch 206, Loss: 0.144189\n",
            "Epoch 207, Loss: 0.144126\n",
            "Epoch 208, Loss: 0.144064\n",
            "Epoch 209, Loss: 0.144002\n",
            "Epoch 210, Loss: 0.143940\n",
            "Epoch 211, Loss: 0.143878\n",
            "Epoch 212, Loss: 0.143816\n",
            "Epoch 213, Loss: 0.143754\n",
            "Epoch 214, Loss: 0.143693\n",
            "Epoch 215, Loss: 0.143632\n",
            "Epoch 216, Loss: 0.143571\n",
            "Epoch 217, Loss: 0.143510\n",
            "Epoch 218, Loss: 0.143449\n",
            "Epoch 219, Loss: 0.143389\n",
            "Epoch 220, Loss: 0.143328\n",
            "Epoch 221, Loss: 0.143268\n",
            "Epoch 222, Loss: 0.143208\n",
            "Epoch 223, Loss: 0.143148\n",
            "Epoch 224, Loss: 0.143088\n",
            "Epoch 225, Loss: 0.143029\n",
            "Epoch 226, Loss: 0.142969\n",
            "Epoch 227, Loss: 0.142910\n",
            "Epoch 228, Loss: 0.142851\n",
            "Epoch 229, Loss: 0.142792\n",
            "Epoch 230, Loss: 0.142733\n",
            "Epoch 231, Loss: 0.142674\n",
            "Epoch 232, Loss: 0.142615\n",
            "Epoch 233, Loss: 0.142557\n",
            "Epoch 234, Loss: 0.142499\n",
            "Epoch 235, Loss: 0.142441\n",
            "Epoch 236, Loss: 0.142383\n",
            "Epoch 237, Loss: 0.142325\n",
            "Epoch 238, Loss: 0.142267\n",
            "Epoch 239, Loss: 0.142210\n",
            "Epoch 240, Loss: 0.142152\n",
            "Epoch 241, Loss: 0.142095\n",
            "Epoch 242, Loss: 0.142038\n",
            "Epoch 243, Loss: 0.141981\n",
            "Epoch 244, Loss: 0.141924\n",
            "Epoch 245, Loss: 0.141868\n",
            "Epoch 246, Loss: 0.141811\n",
            "Epoch 247, Loss: 0.141755\n",
            "Epoch 248, Loss: 0.141699\n",
            "Epoch 249, Loss: 0.141643\n",
            "Epoch 250, Loss: 0.141587\n",
            "Epoch 251, Loss: 0.141531\n",
            "Epoch 252, Loss: 0.141476\n",
            "Epoch 253, Loss: 0.141420\n",
            "Epoch 254, Loss: 0.141365\n",
            "Epoch 255, Loss: 0.141310\n",
            "Epoch 256, Loss: 0.141255\n",
            "Epoch 257, Loss: 0.141200\n",
            "Epoch 258, Loss: 0.141145\n",
            "Epoch 259, Loss: 0.141090\n",
            "Epoch 260, Loss: 0.141036\n",
            "Epoch 261, Loss: 0.140981\n",
            "Epoch 262, Loss: 0.140927\n",
            "Epoch 263, Loss: 0.140873\n",
            "Epoch 264, Loss: 0.140819\n",
            "Epoch 265, Loss: 0.140766\n",
            "Epoch 266, Loss: 0.140712\n",
            "Epoch 267, Loss: 0.140658\n",
            "Epoch 268, Loss: 0.140605\n",
            "Epoch 269, Loss: 0.140552\n",
            "Epoch 270, Loss: 0.140499\n",
            "Epoch 271, Loss: 0.140446\n",
            "Epoch 272, Loss: 0.140393\n",
            "Epoch 273, Loss: 0.140340\n",
            "Epoch 274, Loss: 0.140288\n",
            "Epoch 275, Loss: 0.140235\n",
            "Epoch 276, Loss: 0.140183\n",
            "Epoch 277, Loss: 0.140131\n",
            "Epoch 278, Loss: 0.140079\n",
            "Epoch 279, Loss: 0.140027\n",
            "Epoch 280, Loss: 0.139975\n",
            "Epoch 281, Loss: 0.139923\n",
            "Epoch 282, Loss: 0.139872\n",
            "Epoch 283, Loss: 0.139820\n",
            "Epoch 284, Loss: 0.139769\n",
            "Epoch 285, Loss: 0.139718\n",
            "Epoch 286, Loss: 0.139667\n",
            "Epoch 287, Loss: 0.139616\n",
            "Epoch 288, Loss: 0.139565\n",
            "Epoch 289, Loss: 0.139515\n",
            "Epoch 290, Loss: 0.139464\n",
            "Epoch 291, Loss: 0.139414\n",
            "Epoch 292, Loss: 0.139364\n",
            "Epoch 293, Loss: 0.139314\n",
            "Epoch 294, Loss: 0.139264\n",
            "Epoch 295, Loss: 0.139214\n",
            "Epoch 296, Loss: 0.139164\n",
            "Epoch 297, Loss: 0.139114\n",
            "Epoch 298, Loss: 0.139065\n",
            "Epoch 299, Loss: 0.139016\n",
            "Epoch 300, Loss: 0.138966\n",
            "Epoch 301, Loss: 0.138917\n",
            "Epoch 302, Loss: 0.138868\n",
            "Epoch 303, Loss: 0.138819\n",
            "Epoch 304, Loss: 0.138771\n",
            "Epoch 305, Loss: 0.138722\n",
            "Epoch 306, Loss: 0.138674\n",
            "Epoch 307, Loss: 0.138625\n",
            "Epoch 308, Loss: 0.138577\n",
            "Epoch 309, Loss: 0.138529\n",
            "Epoch 310, Loss: 0.138481\n",
            "Epoch 311, Loss: 0.138433\n",
            "Epoch 312, Loss: 0.138385\n",
            "Epoch 313, Loss: 0.138337\n",
            "Epoch 314, Loss: 0.138290\n",
            "Epoch 315, Loss: 0.138242\n",
            "Epoch 316, Loss: 0.138195\n",
            "Epoch 317, Loss: 0.138148\n",
            "Epoch 318, Loss: 0.138101\n",
            "Epoch 319, Loss: 0.138054\n",
            "Epoch 320, Loss: 0.138007\n",
            "Epoch 321, Loss: 0.137960\n",
            "Epoch 322, Loss: 0.137914\n",
            "Epoch 323, Loss: 0.137867\n",
            "Epoch 324, Loss: 0.137821\n",
            "Epoch 325, Loss: 0.137775\n",
            "Epoch 326, Loss: 0.137728\n",
            "Epoch 327, Loss: 0.137682\n",
            "Epoch 328, Loss: 0.137636\n",
            "Epoch 329, Loss: 0.137591\n",
            "Epoch 330, Loss: 0.137545\n",
            "Epoch 331, Loss: 0.137499\n",
            "Epoch 332, Loss: 0.137454\n",
            "Epoch 333, Loss: 0.137408\n",
            "Epoch 334, Loss: 0.137363\n",
            "Epoch 335, Loss: 0.137318\n",
            "Epoch 336, Loss: 0.137273\n",
            "Epoch 337, Loss: 0.137228\n",
            "Epoch 338, Loss: 0.137183\n",
            "Epoch 339, Loss: 0.137139\n",
            "Epoch 340, Loss: 0.137094\n",
            "Epoch 341, Loss: 0.137049\n",
            "Epoch 342, Loss: 0.137005\n",
            "Epoch 343, Loss: 0.136961\n",
            "Epoch 344, Loss: 0.136917\n",
            "Epoch 345, Loss: 0.136873\n",
            "Epoch 346, Loss: 0.136829\n",
            "Epoch 347, Loss: 0.136785\n",
            "Epoch 348, Loss: 0.136741\n",
            "Epoch 349, Loss: 0.136697\n",
            "Epoch 350, Loss: 0.136654\n",
            "Epoch 351, Loss: 0.136611\n",
            "Epoch 352, Loss: 0.136567\n",
            "Epoch 353, Loss: 0.136524\n",
            "Epoch 354, Loss: 0.136481\n",
            "Epoch 355, Loss: 0.136438\n",
            "Epoch 356, Loss: 0.136395\n",
            "Epoch 357, Loss: 0.136352\n",
            "Epoch 358, Loss: 0.136310\n",
            "Epoch 359, Loss: 0.136267\n",
            "Epoch 360, Loss: 0.136225\n",
            "Epoch 361, Loss: 0.136182\n",
            "Epoch 362, Loss: 0.136140\n",
            "Epoch 363, Loss: 0.136098\n",
            "Epoch 364, Loss: 0.136056\n",
            "Epoch 365, Loss: 0.136014\n",
            "Epoch 366, Loss: 0.135972\n",
            "Epoch 367, Loss: 0.135930\n",
            "Epoch 368, Loss: 0.135888\n",
            "Epoch 369, Loss: 0.135847\n",
            "Epoch 370, Loss: 0.135805\n",
            "Epoch 371, Loss: 0.135764\n",
            "Epoch 372, Loss: 0.135723\n",
            "Epoch 373, Loss: 0.135682\n",
            "Epoch 374, Loss: 0.135641\n",
            "Epoch 375, Loss: 0.135600\n",
            "Epoch 376, Loss: 0.135559\n",
            "Epoch 377, Loss: 0.135518\n",
            "Epoch 378, Loss: 0.135477\n",
            "Epoch 379, Loss: 0.135437\n",
            "Epoch 380, Loss: 0.135396\n",
            "Epoch 381, Loss: 0.135356\n",
            "Epoch 382, Loss: 0.135316\n",
            "Epoch 383, Loss: 0.135275\n",
            "Epoch 384, Loss: 0.135235\n",
            "Epoch 385, Loss: 0.135195\n",
            "Epoch 386, Loss: 0.135155\n",
            "Epoch 387, Loss: 0.135115\n",
            "Epoch 388, Loss: 0.135076\n",
            "Epoch 389, Loss: 0.135036\n",
            "Epoch 390, Loss: 0.134997\n",
            "Epoch 391, Loss: 0.134957\n",
            "Epoch 392, Loss: 0.134918\n",
            "Epoch 393, Loss: 0.134879\n",
            "Epoch 394, Loss: 0.134839\n",
            "Epoch 395, Loss: 0.134800\n",
            "Epoch 396, Loss: 0.134761\n",
            "Epoch 397, Loss: 0.134723\n",
            "Epoch 398, Loss: 0.134684\n",
            "Epoch 399, Loss: 0.134645\n",
            "Epoch 400, Loss: 0.134606\n",
            "Epoch 401, Loss: 0.134568\n",
            "Epoch 402, Loss: 0.134530\n",
            "Epoch 403, Loss: 0.134491\n",
            "Epoch 404, Loss: 0.134453\n",
            "Epoch 405, Loss: 0.134415\n",
            "Epoch 406, Loss: 0.134377\n",
            "Epoch 407, Loss: 0.134339\n",
            "Epoch 408, Loss: 0.134301\n",
            "Epoch 409, Loss: 0.134263\n",
            "Epoch 410, Loss: 0.134225\n",
            "Epoch 411, Loss: 0.134188\n",
            "Epoch 412, Loss: 0.134150\n",
            "Epoch 413, Loss: 0.134113\n",
            "Epoch 414, Loss: 0.134076\n",
            "Epoch 415, Loss: 0.134038\n",
            "Epoch 416, Loss: 0.134001\n",
            "Epoch 417, Loss: 0.133964\n",
            "Epoch 418, Loss: 0.133927\n",
            "Epoch 419, Loss: 0.133890\n",
            "Epoch 420, Loss: 0.133853\n",
            "Epoch 421, Loss: 0.133817\n",
            "Epoch 422, Loss: 0.133780\n",
            "Epoch 423, Loss: 0.133743\n",
            "Epoch 424, Loss: 0.133707\n",
            "Epoch 425, Loss: 0.133670\n",
            "Epoch 426, Loss: 0.133634\n",
            "Epoch 427, Loss: 0.133598\n",
            "Epoch 428, Loss: 0.133562\n",
            "Epoch 429, Loss: 0.133526\n",
            "Epoch 430, Loss: 0.133490\n",
            "Epoch 431, Loss: 0.133454\n",
            "Epoch 432, Loss: 0.133418\n",
            "Epoch 433, Loss: 0.133382\n",
            "Epoch 434, Loss: 0.133347\n",
            "Epoch 435, Loss: 0.133311\n",
            "Epoch 436, Loss: 0.133276\n",
            "Epoch 437, Loss: 0.133240\n",
            "Epoch 438, Loss: 0.133205\n",
            "Epoch 439, Loss: 0.133170\n",
            "Epoch 440, Loss: 0.133135\n",
            "Epoch 441, Loss: 0.133099\n",
            "Epoch 442, Loss: 0.133064\n",
            "Epoch 443, Loss: 0.133030\n",
            "Epoch 444, Loss: 0.132995\n",
            "Epoch 445, Loss: 0.132960\n",
            "Epoch 446, Loss: 0.132925\n",
            "Epoch 447, Loss: 0.132891\n",
            "Epoch 448, Loss: 0.132856\n",
            "Epoch 449, Loss: 0.132822\n",
            "Epoch 450, Loss: 0.132787\n",
            "Epoch 451, Loss: 0.132753\n",
            "Epoch 452, Loss: 0.132719\n",
            "Epoch 453, Loss: 0.132685\n",
            "Epoch 454, Loss: 0.132651\n",
            "Epoch 455, Loss: 0.132617\n",
            "Epoch 456, Loss: 0.132583\n",
            "Epoch 457, Loss: 0.132549\n",
            "Epoch 458, Loss: 0.132516\n",
            "Epoch 459, Loss: 0.132482\n",
            "Epoch 460, Loss: 0.132448\n",
            "Epoch 461, Loss: 0.132415\n",
            "Epoch 462, Loss: 0.132381\n",
            "Epoch 463, Loss: 0.132348\n",
            "Epoch 464, Loss: 0.132315\n",
            "Epoch 465, Loss: 0.132282\n",
            "Epoch 466, Loss: 0.132248\n",
            "Epoch 467, Loss: 0.132215\n",
            "Epoch 468, Loss: 0.132182\n",
            "Epoch 469, Loss: 0.132150\n",
            "Epoch 470, Loss: 0.132117\n",
            "Epoch 471, Loss: 0.132084\n",
            "Epoch 472, Loss: 0.132051\n",
            "Epoch 473, Loss: 0.132019\n",
            "Epoch 474, Loss: 0.131986\n",
            "Epoch 475, Loss: 0.131954\n",
            "Epoch 476, Loss: 0.131921\n",
            "Epoch 477, Loss: 0.131889\n",
            "Epoch 478, Loss: 0.131857\n",
            "Epoch 479, Loss: 0.131825\n",
            "Epoch 480, Loss: 0.131793\n",
            "Epoch 481, Loss: 0.131761\n",
            "Epoch 482, Loss: 0.131729\n",
            "Epoch 483, Loss: 0.131697\n",
            "Epoch 484, Loss: 0.131665\n",
            "Epoch 485, Loss: 0.131633\n",
            "Epoch 486, Loss: 0.131602\n",
            "Epoch 487, Loss: 0.131570\n",
            "Epoch 488, Loss: 0.131539\n",
            "Epoch 489, Loss: 0.131507\n",
            "Epoch 490, Loss: 0.131476\n",
            "Epoch 491, Loss: 0.131445\n",
            "Epoch 492, Loss: 0.131413\n",
            "Epoch 493, Loss: 0.131382\n",
            "Epoch 494, Loss: 0.131351\n",
            "Epoch 495, Loss: 0.131320\n",
            "Epoch 496, Loss: 0.131289\n",
            "Epoch 497, Loss: 0.131258\n",
            "Epoch 498, Loss: 0.131228\n",
            "Epoch 499, Loss: 0.131197\n",
            "Epoch 500, Loss: 0.131166\n",
            "Epoch 501, Loss: 0.131136\n",
            "Epoch 502, Loss: 0.131105\n",
            "Epoch 503, Loss: 0.131075\n",
            "Epoch 504, Loss: 0.131044\n",
            "Epoch 505, Loss: 0.131014\n",
            "Epoch 506, Loss: 0.130984\n",
            "Epoch 507, Loss: 0.130953\n",
            "Epoch 508, Loss: 0.130923\n",
            "Epoch 509, Loss: 0.130893\n",
            "Epoch 510, Loss: 0.130863\n",
            "Epoch 511, Loss: 0.130833\n",
            "Epoch 512, Loss: 0.130803\n",
            "Epoch 513, Loss: 0.130774\n",
            "Epoch 514, Loss: 0.130744\n",
            "Epoch 515, Loss: 0.130714\n",
            "Epoch 516, Loss: 0.130685\n",
            "Epoch 517, Loss: 0.130655\n",
            "Epoch 518, Loss: 0.130626\n",
            "Epoch 519, Loss: 0.130596\n",
            "Epoch 520, Loss: 0.130567\n",
            "Epoch 521, Loss: 0.130538\n",
            "Epoch 522, Loss: 0.130509\n",
            "Epoch 523, Loss: 0.130479\n",
            "Epoch 524, Loss: 0.130450\n",
            "Epoch 525, Loss: 0.130421\n",
            "Epoch 526, Loss: 0.130392\n",
            "Epoch 527, Loss: 0.130364\n",
            "Epoch 528, Loss: 0.130335\n",
            "Epoch 529, Loss: 0.130306\n",
            "Epoch 530, Loss: 0.130277\n",
            "Epoch 531, Loss: 0.130249\n",
            "Epoch 532, Loss: 0.130220\n",
            "Epoch 533, Loss: 0.130192\n",
            "Epoch 534, Loss: 0.130163\n",
            "Epoch 535, Loss: 0.130135\n",
            "Epoch 536, Loss: 0.130106\n",
            "Epoch 537, Loss: 0.130078\n",
            "Epoch 538, Loss: 0.130050\n",
            "Epoch 539, Loss: 0.130022\n",
            "Epoch 540, Loss: 0.129994\n",
            "Epoch 541, Loss: 0.129966\n",
            "Epoch 542, Loss: 0.129938\n",
            "Epoch 543, Loss: 0.129910\n",
            "Epoch 544, Loss: 0.129882\n",
            "Epoch 545, Loss: 0.129854\n",
            "Epoch 546, Loss: 0.129827\n",
            "Epoch 547, Loss: 0.129799\n",
            "Epoch 548, Loss: 0.129771\n",
            "Epoch 549, Loss: 0.129744\n",
            "Epoch 550, Loss: 0.129716\n",
            "Epoch 551, Loss: 0.129689\n",
            "Epoch 552, Loss: 0.129662\n",
            "Epoch 553, Loss: 0.129634\n",
            "Epoch 554, Loss: 0.129607\n",
            "Epoch 555, Loss: 0.129580\n",
            "Epoch 556, Loss: 0.129553\n",
            "Epoch 557, Loss: 0.129526\n",
            "Epoch 558, Loss: 0.129499\n",
            "Epoch 559, Loss: 0.129472\n",
            "Epoch 560, Loss: 0.129445\n",
            "Epoch 561, Loss: 0.129418\n",
            "Epoch 562, Loss: 0.129392\n",
            "Epoch 563, Loss: 0.129365\n",
            "Epoch 564, Loss: 0.129338\n",
            "Epoch 565, Loss: 0.129312\n",
            "Epoch 566, Loss: 0.129285\n",
            "Epoch 567, Loss: 0.129259\n",
            "Epoch 568, Loss: 0.129232\n",
            "Epoch 569, Loss: 0.129206\n",
            "Epoch 570, Loss: 0.129180\n",
            "Epoch 571, Loss: 0.129153\n",
            "Epoch 572, Loss: 0.129127\n",
            "Epoch 573, Loss: 0.129101\n",
            "Epoch 574, Loss: 0.129075\n",
            "Epoch 575, Loss: 0.129049\n",
            "Epoch 576, Loss: 0.129023\n",
            "Epoch 577, Loss: 0.128997\n",
            "Epoch 578, Loss: 0.128971\n",
            "Epoch 579, Loss: 0.128946\n",
            "Epoch 580, Loss: 0.128920\n",
            "Epoch 581, Loss: 0.128894\n",
            "Epoch 582, Loss: 0.128868\n",
            "Epoch 583, Loss: 0.128843\n",
            "Epoch 584, Loss: 0.128817\n",
            "Epoch 585, Loss: 0.128792\n",
            "Epoch 586, Loss: 0.128766\n",
            "Epoch 587, Loss: 0.128741\n",
            "Epoch 588, Loss: 0.128716\n",
            "Epoch 589, Loss: 0.128691\n",
            "Epoch 590, Loss: 0.128665\n",
            "Epoch 591, Loss: 0.128640\n",
            "Epoch 592, Loss: 0.128615\n",
            "Epoch 593, Loss: 0.128590\n",
            "Epoch 594, Loss: 0.128565\n",
            "Epoch 595, Loss: 0.128540\n",
            "Epoch 596, Loss: 0.128515\n",
            "Epoch 597, Loss: 0.128491\n",
            "Epoch 598, Loss: 0.128466\n",
            "Epoch 599, Loss: 0.128441\n",
            "Epoch 600, Loss: 0.128416\n",
            "Epoch 601, Loss: 0.128392\n",
            "Epoch 602, Loss: 0.128367\n",
            "Epoch 603, Loss: 0.128343\n",
            "Epoch 604, Loss: 0.128318\n",
            "Epoch 605, Loss: 0.128294\n",
            "Epoch 606, Loss: 0.128270\n",
            "Epoch 607, Loss: 0.128245\n",
            "Epoch 608, Loss: 0.128221\n",
            "Epoch 609, Loss: 0.128197\n",
            "Epoch 610, Loss: 0.128173\n",
            "Epoch 611, Loss: 0.128149\n",
            "Epoch 612, Loss: 0.128124\n",
            "Epoch 613, Loss: 0.128100\n",
            "Epoch 614, Loss: 0.128077\n",
            "Epoch 615, Loss: 0.128053\n",
            "Epoch 616, Loss: 0.128029\n",
            "Epoch 617, Loss: 0.128005\n",
            "Epoch 618, Loss: 0.127981\n",
            "Epoch 619, Loss: 0.127958\n",
            "Epoch 620, Loss: 0.127934\n",
            "Epoch 621, Loss: 0.127910\n",
            "Epoch 622, Loss: 0.127887\n",
            "Epoch 623, Loss: 0.127863\n",
            "Epoch 624, Loss: 0.127840\n",
            "Epoch 625, Loss: 0.127816\n",
            "Epoch 626, Loss: 0.127793\n",
            "Epoch 627, Loss: 0.127770\n",
            "Epoch 628, Loss: 0.127747\n",
            "Epoch 629, Loss: 0.127723\n",
            "Epoch 630, Loss: 0.127700\n",
            "Epoch 631, Loss: 0.127677\n",
            "Epoch 632, Loss: 0.127654\n",
            "Epoch 633, Loss: 0.127631\n",
            "Epoch 634, Loss: 0.127608\n",
            "Epoch 635, Loss: 0.127585\n",
            "Epoch 636, Loss: 0.127562\n",
            "Epoch 637, Loss: 0.127539\n",
            "Epoch 638, Loss: 0.127517\n",
            "Epoch 639, Loss: 0.127494\n",
            "Epoch 640, Loss: 0.127471\n",
            "Epoch 641, Loss: 0.127449\n",
            "Epoch 642, Loss: 0.127426\n",
            "Epoch 643, Loss: 0.127404\n",
            "Epoch 644, Loss: 0.127381\n",
            "Epoch 645, Loss: 0.127359\n",
            "Epoch 646, Loss: 0.127336\n",
            "Epoch 647, Loss: 0.127314\n",
            "Epoch 648, Loss: 0.127292\n",
            "Epoch 649, Loss: 0.127269\n",
            "Epoch 650, Loss: 0.127247\n",
            "Epoch 651, Loss: 0.127225\n",
            "Epoch 652, Loss: 0.127203\n",
            "Epoch 653, Loss: 0.127181\n",
            "Epoch 654, Loss: 0.127159\n",
            "Epoch 655, Loss: 0.127137\n",
            "Epoch 656, Loss: 0.127115\n",
            "Epoch 657, Loss: 0.127093\n",
            "Epoch 658, Loss: 0.127071\n",
            "Epoch 659, Loss: 0.127049\n",
            "Epoch 660, Loss: 0.127027\n",
            "Epoch 661, Loss: 0.127006\n",
            "Epoch 662, Loss: 0.126984\n",
            "Epoch 663, Loss: 0.126962\n",
            "Epoch 664, Loss: 0.126941\n",
            "Epoch 665, Loss: 0.126919\n",
            "Epoch 666, Loss: 0.126898\n",
            "Epoch 667, Loss: 0.126876\n",
            "Epoch 668, Loss: 0.126855\n",
            "Epoch 669, Loss: 0.126834\n",
            "Epoch 670, Loss: 0.126812\n",
            "Epoch 671, Loss: 0.126791\n",
            "Epoch 672, Loss: 0.126770\n",
            "Epoch 673, Loss: 0.126749\n",
            "Epoch 674, Loss: 0.126727\n",
            "Epoch 675, Loss: 0.126706\n",
            "Epoch 676, Loss: 0.126685\n",
            "Epoch 677, Loss: 0.126664\n",
            "Epoch 678, Loss: 0.126643\n",
            "Epoch 679, Loss: 0.126622\n",
            "Epoch 680, Loss: 0.126601\n",
            "Epoch 681, Loss: 0.126581\n",
            "Epoch 682, Loss: 0.126560\n",
            "Epoch 683, Loss: 0.126539\n",
            "Epoch 684, Loss: 0.126518\n",
            "Epoch 685, Loss: 0.126498\n",
            "Epoch 686, Loss: 0.126477\n",
            "Epoch 687, Loss: 0.126456\n",
            "Epoch 688, Loss: 0.126436\n",
            "Epoch 689, Loss: 0.126415\n",
            "Epoch 690, Loss: 0.126395\n",
            "Epoch 691, Loss: 0.126374\n",
            "Epoch 692, Loss: 0.126354\n",
            "Epoch 693, Loss: 0.126334\n",
            "Epoch 694, Loss: 0.126313\n",
            "Epoch 695, Loss: 0.126293\n",
            "Epoch 696, Loss: 0.126273\n",
            "Epoch 697, Loss: 0.126253\n",
            "Epoch 698, Loss: 0.126233\n",
            "Epoch 699, Loss: 0.126212\n",
            "Epoch 700, Loss: 0.126192\n",
            "Epoch 701, Loss: 0.126172\n",
            "Epoch 702, Loss: 0.126152\n",
            "Epoch 703, Loss: 0.126132\n",
            "Epoch 704, Loss: 0.126113\n",
            "Epoch 705, Loss: 0.126093\n",
            "Epoch 706, Loss: 0.126073\n",
            "Epoch 707, Loss: 0.126053\n",
            "Epoch 708, Loss: 0.126033\n",
            "Epoch 709, Loss: 0.126014\n",
            "Epoch 710, Loss: 0.125994\n",
            "Epoch 711, Loss: 0.125974\n",
            "Epoch 712, Loss: 0.125955\n",
            "Epoch 713, Loss: 0.125935\n",
            "Epoch 714, Loss: 0.125916\n",
            "Epoch 715, Loss: 0.125896\n",
            "Epoch 716, Loss: 0.125877\n",
            "Epoch 717, Loss: 0.125857\n",
            "Epoch 718, Loss: 0.125838\n",
            "Epoch 719, Loss: 0.125819\n",
            "Epoch 720, Loss: 0.125799\n",
            "Epoch 721, Loss: 0.125780\n",
            "Epoch 722, Loss: 0.125761\n",
            "Epoch 723, Loss: 0.125742\n",
            "Epoch 724, Loss: 0.125723\n",
            "Epoch 725, Loss: 0.125704\n",
            "Epoch 726, Loss: 0.125685\n",
            "Epoch 727, Loss: 0.125666\n",
            "Epoch 728, Loss: 0.125647\n",
            "Epoch 729, Loss: 0.125628\n",
            "Epoch 730, Loss: 0.125609\n",
            "Epoch 731, Loss: 0.125590\n",
            "Epoch 732, Loss: 0.125571\n",
            "Epoch 733, Loss: 0.125552\n",
            "Epoch 734, Loss: 0.125534\n",
            "Epoch 735, Loss: 0.125515\n",
            "Epoch 736, Loss: 0.125496\n",
            "Epoch 737, Loss: 0.125477\n",
            "Epoch 738, Loss: 0.125459\n",
            "Epoch 739, Loss: 0.125440\n",
            "Epoch 740, Loss: 0.125422\n",
            "Epoch 741, Loss: 0.125403\n",
            "Epoch 742, Loss: 0.125385\n",
            "Epoch 743, Loss: 0.125366\n",
            "Epoch 744, Loss: 0.125348\n",
            "Epoch 745, Loss: 0.125330\n",
            "Epoch 746, Loss: 0.125311\n",
            "Epoch 747, Loss: 0.125293\n",
            "Epoch 748, Loss: 0.125275\n",
            "Epoch 749, Loss: 0.125257\n",
            "Epoch 750, Loss: 0.125239\n",
            "Epoch 751, Loss: 0.125220\n",
            "Epoch 752, Loss: 0.125202\n",
            "Epoch 753, Loss: 0.125184\n",
            "Epoch 754, Loss: 0.125166\n",
            "Epoch 755, Loss: 0.125148\n",
            "Epoch 756, Loss: 0.125130\n",
            "Epoch 757, Loss: 0.125112\n",
            "Epoch 758, Loss: 0.125094\n",
            "Epoch 759, Loss: 0.125077\n",
            "Epoch 760, Loss: 0.125059\n",
            "Epoch 761, Loss: 0.125041\n",
            "Epoch 762, Loss: 0.125023\n",
            "Epoch 763, Loss: 0.125006\n",
            "Epoch 764, Loss: 0.124988\n",
            "Epoch 765, Loss: 0.124970\n",
            "Epoch 766, Loss: 0.124953\n",
            "Epoch 767, Loss: 0.124935\n",
            "Epoch 768, Loss: 0.124917\n",
            "Epoch 769, Loss: 0.124900\n",
            "Epoch 770, Loss: 0.124882\n",
            "Epoch 771, Loss: 0.124865\n",
            "Epoch 772, Loss: 0.124848\n",
            "Epoch 773, Loss: 0.124830\n",
            "Epoch 774, Loss: 0.124813\n",
            "Epoch 775, Loss: 0.124796\n",
            "Epoch 776, Loss: 0.124778\n",
            "Epoch 777, Loss: 0.124761\n",
            "Epoch 778, Loss: 0.124744\n",
            "Epoch 779, Loss: 0.124727\n",
            "Epoch 780, Loss: 0.124710\n",
            "Epoch 781, Loss: 0.124692\n",
            "Epoch 782, Loss: 0.124675\n",
            "Epoch 783, Loss: 0.124658\n",
            "Epoch 784, Loss: 0.124641\n",
            "Epoch 785, Loss: 0.124624\n",
            "Epoch 786, Loss: 0.124607\n",
            "Epoch 787, Loss: 0.124591\n",
            "Epoch 788, Loss: 0.124574\n",
            "Epoch 789, Loss: 0.124557\n",
            "Epoch 790, Loss: 0.124540\n",
            "Epoch 791, Loss: 0.124523\n",
            "Epoch 792, Loss: 0.124506\n",
            "Epoch 793, Loss: 0.124490\n",
            "Epoch 794, Loss: 0.124473\n",
            "Epoch 795, Loss: 0.124456\n",
            "Epoch 796, Loss: 0.124440\n",
            "Epoch 797, Loss: 0.124423\n",
            "Epoch 798, Loss: 0.124407\n",
            "Epoch 799, Loss: 0.124390\n",
            "Epoch 800, Loss: 0.124374\n",
            "Epoch 801, Loss: 0.124357\n",
            "Epoch 802, Loss: 0.124341\n",
            "Epoch 803, Loss: 0.124324\n",
            "Epoch 804, Loss: 0.124308\n",
            "Epoch 805, Loss: 0.124292\n",
            "Epoch 806, Loss: 0.124275\n",
            "Epoch 807, Loss: 0.124259\n",
            "Epoch 808, Loss: 0.124243\n",
            "Epoch 809, Loss: 0.124227\n",
            "Epoch 810, Loss: 0.124210\n",
            "Epoch 811, Loss: 0.124194\n",
            "Epoch 812, Loss: 0.124178\n",
            "Epoch 813, Loss: 0.124162\n",
            "Epoch 814, Loss: 0.124146\n",
            "Epoch 815, Loss: 0.124130\n",
            "Epoch 816, Loss: 0.124114\n",
            "Epoch 817, Loss: 0.124098\n",
            "Epoch 818, Loss: 0.124082\n",
            "Epoch 819, Loss: 0.124066\n",
            "Epoch 820, Loss: 0.124050\n",
            "Epoch 821, Loss: 0.124034\n",
            "Epoch 822, Loss: 0.124019\n",
            "Epoch 823, Loss: 0.124003\n",
            "Epoch 824, Loss: 0.123987\n",
            "Epoch 825, Loss: 0.123971\n",
            "Epoch 826, Loss: 0.123956\n",
            "Epoch 827, Loss: 0.123940\n",
            "Epoch 828, Loss: 0.123924\n",
            "Epoch 829, Loss: 0.123909\n",
            "Epoch 830, Loss: 0.123893\n",
            "Epoch 831, Loss: 0.123878\n",
            "Epoch 832, Loss: 0.123862\n",
            "Epoch 833, Loss: 0.123847\n",
            "Epoch 834, Loss: 0.123831\n",
            "Epoch 835, Loss: 0.123816\n",
            "Epoch 836, Loss: 0.123800\n",
            "Epoch 837, Loss: 0.123785\n",
            "Epoch 838, Loss: 0.123770\n",
            "Epoch 839, Loss: 0.123754\n",
            "Epoch 840, Loss: 0.123739\n",
            "Epoch 841, Loss: 0.123724\n",
            "Epoch 842, Loss: 0.123708\n",
            "Epoch 843, Loss: 0.123693\n",
            "Epoch 844, Loss: 0.123678\n",
            "Epoch 845, Loss: 0.123663\n",
            "Epoch 846, Loss: 0.123648\n",
            "Epoch 847, Loss: 0.123633\n",
            "Epoch 848, Loss: 0.123618\n",
            "Epoch 849, Loss: 0.123603\n",
            "Epoch 850, Loss: 0.123588\n",
            "Epoch 851, Loss: 0.123573\n",
            "Epoch 852, Loss: 0.123558\n",
            "Epoch 853, Loss: 0.123543\n",
            "Epoch 854, Loss: 0.123528\n",
            "Epoch 855, Loss: 0.123513\n",
            "Epoch 856, Loss: 0.123498\n",
            "Epoch 857, Loss: 0.123483\n",
            "Epoch 858, Loss: 0.123468\n",
            "Epoch 859, Loss: 0.123454\n",
            "Epoch 860, Loss: 0.123439\n",
            "Epoch 861, Loss: 0.123424\n",
            "Epoch 862, Loss: 0.123410\n",
            "Epoch 863, Loss: 0.123395\n",
            "Epoch 864, Loss: 0.123380\n",
            "Epoch 865, Loss: 0.123366\n",
            "Epoch 866, Loss: 0.123351\n",
            "Epoch 867, Loss: 0.123337\n",
            "Epoch 868, Loss: 0.123322\n",
            "Epoch 869, Loss: 0.123308\n",
            "Epoch 870, Loss: 0.123293\n",
            "Epoch 871, Loss: 0.123279\n",
            "Epoch 872, Loss: 0.123264\n",
            "Epoch 873, Loss: 0.123250\n",
            "Epoch 874, Loss: 0.123236\n",
            "Epoch 875, Loss: 0.123221\n",
            "Epoch 876, Loss: 0.123207\n",
            "Epoch 877, Loss: 0.123193\n",
            "Epoch 878, Loss: 0.123178\n",
            "Epoch 879, Loss: 0.123164\n",
            "Epoch 880, Loss: 0.123150\n",
            "Epoch 881, Loss: 0.123136\n",
            "Epoch 882, Loss: 0.123122\n",
            "Epoch 883, Loss: 0.123107\n",
            "Epoch 884, Loss: 0.123093\n",
            "Epoch 885, Loss: 0.123079\n",
            "Epoch 886, Loss: 0.123065\n",
            "Epoch 887, Loss: 0.123051\n",
            "Epoch 888, Loss: 0.123037\n",
            "Epoch 889, Loss: 0.123023\n",
            "Epoch 890, Loss: 0.123009\n",
            "Epoch 891, Loss: 0.122995\n",
            "Epoch 892, Loss: 0.122982\n",
            "Epoch 893, Loss: 0.122968\n",
            "Epoch 894, Loss: 0.122954\n",
            "Epoch 895, Loss: 0.122940\n",
            "Epoch 896, Loss: 0.122926\n",
            "Epoch 897, Loss: 0.122912\n",
            "Epoch 898, Loss: 0.122899\n",
            "Epoch 899, Loss: 0.122885\n",
            "Epoch 900, Loss: 0.122871\n",
            "Epoch 901, Loss: 0.122858\n",
            "Epoch 902, Loss: 0.122844\n",
            "Epoch 903, Loss: 0.122830\n",
            "Epoch 904, Loss: 0.122817\n",
            "Epoch 905, Loss: 0.122803\n",
            "Epoch 906, Loss: 0.122790\n",
            "Epoch 907, Loss: 0.122776\n",
            "Epoch 908, Loss: 0.122763\n",
            "Epoch 909, Loss: 0.122749\n",
            "Epoch 910, Loss: 0.122736\n",
            "Epoch 911, Loss: 0.122722\n",
            "Epoch 912, Loss: 0.122709\n",
            "Epoch 913, Loss: 0.122696\n",
            "Epoch 914, Loss: 0.122682\n",
            "Epoch 915, Loss: 0.122669\n",
            "Epoch 916, Loss: 0.122656\n",
            "Epoch 917, Loss: 0.122642\n",
            "Epoch 918, Loss: 0.122629\n",
            "Epoch 919, Loss: 0.122616\n",
            "Epoch 920, Loss: 0.122603\n",
            "Epoch 921, Loss: 0.122589\n",
            "Epoch 922, Loss: 0.122576\n",
            "Epoch 923, Loss: 0.122563\n",
            "Epoch 924, Loss: 0.122550\n",
            "Epoch 925, Loss: 0.122537\n",
            "Epoch 926, Loss: 0.122524\n",
            "Epoch 927, Loss: 0.122511\n",
            "Epoch 928, Loss: 0.122498\n",
            "Epoch 929, Loss: 0.122485\n",
            "Epoch 930, Loss: 0.122472\n",
            "Epoch 931, Loss: 0.122459\n",
            "Epoch 932, Loss: 0.122446\n",
            "Epoch 933, Loss: 0.122433\n",
            "Epoch 934, Loss: 0.122420\n",
            "Epoch 935, Loss: 0.122407\n",
            "Epoch 936, Loss: 0.122395\n",
            "Epoch 937, Loss: 0.122382\n",
            "Epoch 938, Loss: 0.122369\n",
            "Epoch 939, Loss: 0.122356\n",
            "Epoch 940, Loss: 0.122343\n",
            "Epoch 941, Loss: 0.122331\n",
            "Epoch 942, Loss: 0.122318\n",
            "Epoch 943, Loss: 0.122305\n",
            "Epoch 944, Loss: 0.122293\n",
            "Epoch 945, Loss: 0.122280\n",
            "Epoch 946, Loss: 0.122268\n",
            "Epoch 947, Loss: 0.122255\n",
            "Epoch 948, Loss: 0.122242\n",
            "Epoch 949, Loss: 0.122230\n",
            "Epoch 950, Loss: 0.122217\n",
            "Epoch 951, Loss: 0.122205\n",
            "Epoch 952, Loss: 0.122192\n",
            "Epoch 953, Loss: 0.122180\n",
            "Epoch 954, Loss: 0.122167\n",
            "Epoch 955, Loss: 0.122155\n",
            "Epoch 956, Loss: 0.122143\n",
            "Epoch 957, Loss: 0.122130\n",
            "Epoch 958, Loss: 0.122118\n",
            "Epoch 959, Loss: 0.122106\n",
            "Epoch 960, Loss: 0.122093\n",
            "Epoch 961, Loss: 0.122081\n",
            "Epoch 962, Loss: 0.122069\n",
            "Epoch 963, Loss: 0.122057\n",
            "Epoch 964, Loss: 0.122044\n",
            "Epoch 965, Loss: 0.122032\n",
            "Epoch 966, Loss: 0.122020\n",
            "Epoch 967, Loss: 0.122008\n",
            "Epoch 968, Loss: 0.121996\n",
            "Epoch 969, Loss: 0.121984\n",
            "Epoch 970, Loss: 0.121972\n",
            "Epoch 971, Loss: 0.121960\n",
            "Epoch 972, Loss: 0.121947\n",
            "Epoch 973, Loss: 0.121935\n",
            "Epoch 974, Loss: 0.121923\n",
            "Epoch 975, Loss: 0.121911\n",
            "Epoch 976, Loss: 0.121900\n",
            "Epoch 977, Loss: 0.121888\n",
            "Epoch 978, Loss: 0.121876\n",
            "Epoch 979, Loss: 0.121864\n",
            "Epoch 980, Loss: 0.121852\n",
            "Epoch 981, Loss: 0.121840\n",
            "Epoch 982, Loss: 0.121828\n",
            "Epoch 983, Loss: 0.121816\n",
            "Epoch 984, Loss: 0.121805\n",
            "Epoch 985, Loss: 0.121793\n",
            "Epoch 986, Loss: 0.121781\n",
            "Epoch 987, Loss: 0.121769\n",
            "Epoch 988, Loss: 0.121758\n",
            "Epoch 989, Loss: 0.121746\n",
            "Epoch 990, Loss: 0.121734\n",
            "Epoch 991, Loss: 0.121723\n",
            "Epoch 992, Loss: 0.121711\n",
            "Epoch 993, Loss: 0.121699\n",
            "Epoch 994, Loss: 0.121688\n",
            "Epoch 995, Loss: 0.121676\n",
            "Epoch 996, Loss: 0.121665\n",
            "Epoch 997, Loss: 0.121653\n",
            "Epoch 998, Loss: 0.121642\n",
            "Epoch 999, Loss: 0.121630\n",
            "Epoch 1000, Loss: 0.121619\n",
            "\n",
            "Test Accuracy: 86.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Credit Risk Assessment"
      ],
      "metadata": {
        "id": "Cq-n35okUJSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# --- Data Loading ---\n",
        "loan_path = '/content/drive/MyDrive/Colab Notebooks/loan_data.csv'\n",
        "loan_data = pd.read_csv(loan_path).dropna()\n",
        "\n",
        "loan_data = pd.get_dummies(\n",
        "    loan_data,\n",
        "    columns=['person_gender', 'person_education', 'person_home_ownership',\n",
        "             'loan_intent', 'previous_loan_defaults_on_file'],\n",
        "    drop_first=True\n",
        ")\n",
        "\n",
        "X = loan_data.drop(\"loan_status\", axis=1).values\n",
        "y = loan_data[\"loan_status\"].values.reshape(-1, 1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# --- Parameters ---\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "lr = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# --- Training ---\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    z1 = X_train @ W1 + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = a1 @ W2 + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # BCE Loss\n",
        "    loss = -np.mean(y_train * np.log(a2 + 1e-8) + (1 - y_train) * np.log(1 - a2 + 1e-8))\n",
        "\n",
        "    # Backprop\n",
        "    d_out = (a2 - y_train) / len(y_train)\n",
        "\n",
        "    dW2 = a1.T @ d_out\n",
        "    db2 = np.sum(d_out, axis=0, keepdims=True)\n",
        "\n",
        "    d_hidden = d_out @ W2.T * relu_derivative(a1)\n",
        "    dW1 = X_train.T @ d_hidden\n",
        "    db1 = np.sum(d_hidden, axis=0, keepdims=True)\n",
        "\n",
        "    dW2 = np.clip(dW2, -1, 1)\n",
        "    db2 = np.clip(db2, -1, 1)\n",
        "    dW1 = np.clip(dW1, -1, 1)\n",
        "    db1 = np.clip(db1, -1, 1)\n",
        "\n",
        "    W2 -= lr * dW2\n",
        "    b2 -= lr * db2\n",
        "    W1 -= lr * dW1\n",
        "    b1 -= lr * db1\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss:.6f}\")\n",
        "\n",
        "# --- Prediction ---\n",
        "def predict(X):\n",
        "    a1 = relu(X @ W1 + b1)\n",
        "    a2 = sigmoid(a1 @ W2 + b2)\n",
        "    return (a2 >= 0.5).astype(int)\n",
        "\n",
        "# --- Evaluate ---\n",
        "y_pred = predict(X_test)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "wsWcZr_6c-dB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96426f40-4850-4e3b-854f-6670cd00845f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.018562\n",
            "Epoch 2, Loss: 1.011964\n",
            "Epoch 3, Loss: 1.005461\n",
            "Epoch 4, Loss: 0.999052\n",
            "Epoch 5, Loss: 0.992735\n",
            "Epoch 6, Loss: 0.986510\n",
            "Epoch 7, Loss: 0.980374\n",
            "Epoch 8, Loss: 0.974326\n",
            "Epoch 9, Loss: 0.968366\n",
            "Epoch 10, Loss: 0.962490\n",
            "Epoch 11, Loss: 0.956698\n",
            "Epoch 12, Loss: 0.950989\n",
            "Epoch 13, Loss: 0.945361\n",
            "Epoch 14, Loss: 0.939812\n",
            "Epoch 15, Loss: 0.934343\n",
            "Epoch 16, Loss: 0.928952\n",
            "Epoch 17, Loss: 0.923637\n",
            "Epoch 18, Loss: 0.918397\n",
            "Epoch 19, Loss: 0.913232\n",
            "Epoch 20, Loss: 0.908140\n",
            "Epoch 21, Loss: 0.903120\n",
            "Epoch 22, Loss: 0.898171\n",
            "Epoch 23, Loss: 0.893291\n",
            "Epoch 24, Loss: 0.888480\n",
            "Epoch 25, Loss: 0.883736\n",
            "Epoch 26, Loss: 0.879059\n",
            "Epoch 27, Loss: 0.874448\n",
            "Epoch 28, Loss: 0.869901\n",
            "Epoch 29, Loss: 0.865417\n",
            "Epoch 30, Loss: 0.860996\n",
            "Epoch 31, Loss: 0.856637\n",
            "Epoch 32, Loss: 0.852337\n",
            "Epoch 33, Loss: 0.848098\n",
            "Epoch 34, Loss: 0.843917\n",
            "Epoch 35, Loss: 0.839793\n",
            "Epoch 36, Loss: 0.835726\n",
            "Epoch 37, Loss: 0.831715\n",
            "Epoch 38, Loss: 0.827760\n",
            "Epoch 39, Loss: 0.823858\n",
            "Epoch 40, Loss: 0.820009\n",
            "Epoch 41, Loss: 0.816213\n",
            "Epoch 42, Loss: 0.812469\n",
            "Epoch 43, Loss: 0.808776\n",
            "Epoch 44, Loss: 0.805133\n",
            "Epoch 45, Loss: 0.801539\n",
            "Epoch 46, Loss: 0.797994\n",
            "Epoch 47, Loss: 0.794496\n",
            "Epoch 48, Loss: 0.791045\n",
            "Epoch 49, Loss: 0.787641\n",
            "Epoch 50, Loss: 0.784282\n",
            "Epoch 51, Loss: 0.780968\n",
            "Epoch 52, Loss: 0.777699\n",
            "Epoch 53, Loss: 0.774473\n",
            "Epoch 54, Loss: 0.771289\n",
            "Epoch 55, Loss: 0.768148\n",
            "Epoch 56, Loss: 0.765049\n",
            "Epoch 57, Loss: 0.761990\n",
            "Epoch 58, Loss: 0.758972\n",
            "Epoch 59, Loss: 0.755994\n",
            "Epoch 60, Loss: 0.753055\n",
            "Epoch 61, Loss: 0.750154\n",
            "Epoch 62, Loss: 0.747291\n",
            "Epoch 63, Loss: 0.744465\n",
            "Epoch 64, Loss: 0.741676\n",
            "Epoch 65, Loss: 0.738923\n",
            "Epoch 66, Loss: 0.736206\n",
            "Epoch 67, Loss: 0.733525\n",
            "Epoch 68, Loss: 0.730878\n",
            "Epoch 69, Loss: 0.728265\n",
            "Epoch 70, Loss: 0.725686\n",
            "Epoch 71, Loss: 0.723140\n",
            "Epoch 72, Loss: 0.720627\n",
            "Epoch 73, Loss: 0.718146\n",
            "Epoch 74, Loss: 0.715696\n",
            "Epoch 75, Loss: 0.713278\n",
            "Epoch 76, Loss: 0.710891\n",
            "Epoch 77, Loss: 0.708533\n",
            "Epoch 78, Loss: 0.706206\n",
            "Epoch 79, Loss: 0.703908\n",
            "Epoch 80, Loss: 0.701639\n",
            "Epoch 81, Loss: 0.699399\n",
            "Epoch 82, Loss: 0.697187\n",
            "Epoch 83, Loss: 0.695002\n",
            "Epoch 84, Loss: 0.692845\n",
            "Epoch 85, Loss: 0.690716\n",
            "Epoch 86, Loss: 0.688612\n",
            "Epoch 87, Loss: 0.686535\n",
            "Epoch 88, Loss: 0.684484\n",
            "Epoch 89, Loss: 0.682459\n",
            "Epoch 90, Loss: 0.680459\n",
            "Epoch 91, Loss: 0.678484\n",
            "Epoch 92, Loss: 0.676534\n",
            "Epoch 93, Loss: 0.674608\n",
            "Epoch 94, Loss: 0.672706\n",
            "Epoch 95, Loss: 0.670828\n",
            "Epoch 96, Loss: 0.668973\n",
            "Epoch 97, Loss: 0.667141\n",
            "Epoch 98, Loss: 0.665332\n",
            "Epoch 99, Loss: 0.663545\n",
            "Epoch 100, Loss: 0.661780\n",
            "Epoch 101, Loss: 0.660037\n",
            "Epoch 102, Loss: 0.658315\n",
            "Epoch 103, Loss: 0.656615\n",
            "Epoch 104, Loss: 0.654935\n",
            "Epoch 105, Loss: 0.653276\n",
            "Epoch 106, Loss: 0.651637\n",
            "Epoch 107, Loss: 0.650018\n",
            "Epoch 108, Loss: 0.648418\n",
            "Epoch 109, Loss: 0.646838\n",
            "Epoch 110, Loss: 0.645276\n",
            "Epoch 111, Loss: 0.643733\n",
            "Epoch 112, Loss: 0.642208\n",
            "Epoch 113, Loss: 0.640701\n",
            "Epoch 114, Loss: 0.639212\n",
            "Epoch 115, Loss: 0.637741\n",
            "Epoch 116, Loss: 0.636286\n",
            "Epoch 117, Loss: 0.634849\n",
            "Epoch 118, Loss: 0.633428\n",
            "Epoch 119, Loss: 0.632023\n",
            "Epoch 120, Loss: 0.630635\n",
            "Epoch 121, Loss: 0.629263\n",
            "Epoch 122, Loss: 0.627906\n",
            "Epoch 123, Loss: 0.626564\n",
            "Epoch 124, Loss: 0.625237\n",
            "Epoch 125, Loss: 0.623926\n",
            "Epoch 126, Loss: 0.622629\n",
            "Epoch 127, Loss: 0.621347\n",
            "Epoch 128, Loss: 0.620079\n",
            "Epoch 129, Loss: 0.618824\n",
            "Epoch 130, Loss: 0.617584\n",
            "Epoch 131, Loss: 0.616357\n",
            "Epoch 132, Loss: 0.615144\n",
            "Epoch 133, Loss: 0.613944\n",
            "Epoch 134, Loss: 0.612757\n",
            "Epoch 135, Loss: 0.611583\n",
            "Epoch 136, Loss: 0.610421\n",
            "Epoch 137, Loss: 0.609272\n",
            "Epoch 138, Loss: 0.608135\n",
            "Epoch 139, Loss: 0.607010\n",
            "Epoch 140, Loss: 0.605897\n",
            "Epoch 141, Loss: 0.604796\n",
            "Epoch 142, Loss: 0.603707\n",
            "Epoch 143, Loss: 0.602628\n",
            "Epoch 144, Loss: 0.601561\n",
            "Epoch 145, Loss: 0.600506\n",
            "Epoch 146, Loss: 0.599461\n",
            "Epoch 147, Loss: 0.598426\n",
            "Epoch 148, Loss: 0.597403\n",
            "Epoch 149, Loss: 0.596390\n",
            "Epoch 150, Loss: 0.595387\n",
            "Epoch 151, Loss: 0.594395\n",
            "Epoch 152, Loss: 0.593412\n",
            "Epoch 153, Loss: 0.592439\n",
            "Epoch 154, Loss: 0.591477\n",
            "Epoch 155, Loss: 0.590523\n",
            "Epoch 156, Loss: 0.589580\n",
            "Epoch 157, Loss: 0.588645\n",
            "Epoch 158, Loss: 0.587720\n",
            "Epoch 159, Loss: 0.586804\n",
            "Epoch 160, Loss: 0.585897\n",
            "Epoch 161, Loss: 0.584999\n",
            "Epoch 162, Loss: 0.584110\n",
            "Epoch 163, Loss: 0.583229\n",
            "Epoch 164, Loss: 0.582357\n",
            "Epoch 165, Loss: 0.581493\n",
            "Epoch 166, Loss: 0.580638\n",
            "Epoch 167, Loss: 0.579790\n",
            "Epoch 168, Loss: 0.578951\n",
            "Epoch 169, Loss: 0.578120\n",
            "Epoch 170, Loss: 0.577296\n",
            "Epoch 171, Loss: 0.576481\n",
            "Epoch 172, Loss: 0.575672\n",
            "Epoch 173, Loss: 0.574872\n",
            "Epoch 174, Loss: 0.574079\n",
            "Epoch 175, Loss: 0.573293\n",
            "Epoch 176, Loss: 0.572514\n",
            "Epoch 177, Loss: 0.571743\n",
            "Epoch 178, Loss: 0.570979\n",
            "Epoch 179, Loss: 0.570221\n",
            "Epoch 180, Loss: 0.569471\n",
            "Epoch 181, Loss: 0.568727\n",
            "Epoch 182, Loss: 0.567990\n",
            "Epoch 183, Loss: 0.567260\n",
            "Epoch 184, Loss: 0.566536\n",
            "Epoch 185, Loss: 0.565819\n",
            "Epoch 186, Loss: 0.565108\n",
            "Epoch 187, Loss: 0.564403\n",
            "Epoch 188, Loss: 0.563705\n",
            "Epoch 189, Loss: 0.563012\n",
            "Epoch 190, Loss: 0.562326\n",
            "Epoch 191, Loss: 0.561646\n",
            "Epoch 192, Loss: 0.560971\n",
            "Epoch 193, Loss: 0.560303\n",
            "Epoch 194, Loss: 0.559640\n",
            "Epoch 195, Loss: 0.558983\n",
            "Epoch 196, Loss: 0.558331\n",
            "Epoch 197, Loss: 0.557685\n",
            "Epoch 198, Loss: 0.557044\n",
            "Epoch 199, Loss: 0.556409\n",
            "Epoch 200, Loss: 0.555779\n",
            "Epoch 201, Loss: 0.555154\n",
            "Epoch 202, Loss: 0.554535\n",
            "Epoch 203, Loss: 0.553921\n",
            "Epoch 204, Loss: 0.553311\n",
            "Epoch 205, Loss: 0.552707\n",
            "Epoch 206, Loss: 0.552107\n",
            "Epoch 207, Loss: 0.551513\n",
            "Epoch 208, Loss: 0.550923\n",
            "Epoch 209, Loss: 0.550338\n",
            "Epoch 210, Loss: 0.549758\n",
            "Epoch 211, Loss: 0.549183\n",
            "Epoch 212, Loss: 0.548612\n",
            "Epoch 213, Loss: 0.548046\n",
            "Epoch 214, Loss: 0.547484\n",
            "Epoch 215, Loss: 0.546926\n",
            "Epoch 216, Loss: 0.546373\n",
            "Epoch 217, Loss: 0.545825\n",
            "Epoch 218, Loss: 0.545280\n",
            "Epoch 219, Loss: 0.544740\n",
            "Epoch 220, Loss: 0.544204\n",
            "Epoch 221, Loss: 0.543673\n",
            "Epoch 222, Loss: 0.543145\n",
            "Epoch 223, Loss: 0.542621\n",
            "Epoch 224, Loss: 0.542102\n",
            "Epoch 225, Loss: 0.541586\n",
            "Epoch 226, Loss: 0.541074\n",
            "Epoch 227, Loss: 0.540566\n",
            "Epoch 228, Loss: 0.540062\n",
            "Epoch 229, Loss: 0.539562\n",
            "Epoch 230, Loss: 0.539065\n",
            "Epoch 231, Loss: 0.538572\n",
            "Epoch 232, Loss: 0.538083\n",
            "Epoch 233, Loss: 0.537597\n",
            "Epoch 234, Loss: 0.537115\n",
            "Epoch 235, Loss: 0.536636\n",
            "Epoch 236, Loss: 0.536161\n",
            "Epoch 237, Loss: 0.535689\n",
            "Epoch 238, Loss: 0.535220\n",
            "Epoch 239, Loss: 0.534755\n",
            "Epoch 240, Loss: 0.534293\n",
            "Epoch 241, Loss: 0.533834\n",
            "Epoch 242, Loss: 0.533379\n",
            "Epoch 243, Loss: 0.532926\n",
            "Epoch 244, Loss: 0.532477\n",
            "Epoch 245, Loss: 0.532031\n",
            "Epoch 246, Loss: 0.531589\n",
            "Epoch 247, Loss: 0.531149\n",
            "Epoch 248, Loss: 0.530713\n",
            "Epoch 249, Loss: 0.530279\n",
            "Epoch 250, Loss: 0.529848\n",
            "Epoch 251, Loss: 0.529421\n",
            "Epoch 252, Loss: 0.528996\n",
            "Epoch 253, Loss: 0.528574\n",
            "Epoch 254, Loss: 0.528155\n",
            "Epoch 255, Loss: 0.527738\n",
            "Epoch 256, Loss: 0.527324\n",
            "Epoch 257, Loss: 0.526913\n",
            "Epoch 258, Loss: 0.526505\n",
            "Epoch 259, Loss: 0.526099\n",
            "Epoch 260, Loss: 0.525696\n",
            "Epoch 261, Loss: 0.525296\n",
            "Epoch 262, Loss: 0.524898\n",
            "Epoch 263, Loss: 0.524503\n",
            "Epoch 264, Loss: 0.524111\n",
            "Epoch 265, Loss: 0.523721\n",
            "Epoch 266, Loss: 0.523333\n",
            "Epoch 267, Loss: 0.522948\n",
            "Epoch 268, Loss: 0.522565\n",
            "Epoch 269, Loss: 0.522185\n",
            "Epoch 270, Loss: 0.521807\n",
            "Epoch 271, Loss: 0.521432\n",
            "Epoch 272, Loss: 0.521059\n",
            "Epoch 273, Loss: 0.520688\n",
            "Epoch 274, Loss: 0.520319\n",
            "Epoch 275, Loss: 0.519953\n",
            "Epoch 276, Loss: 0.519589\n",
            "Epoch 277, Loss: 0.519227\n",
            "Epoch 278, Loss: 0.518867\n",
            "Epoch 279, Loss: 0.518510\n",
            "Epoch 280, Loss: 0.518155\n",
            "Epoch 281, Loss: 0.517802\n",
            "Epoch 282, Loss: 0.517451\n",
            "Epoch 283, Loss: 0.517102\n",
            "Epoch 284, Loss: 0.516755\n",
            "Epoch 285, Loss: 0.516410\n",
            "Epoch 286, Loss: 0.516068\n",
            "Epoch 287, Loss: 0.515727\n",
            "Epoch 288, Loss: 0.515388\n",
            "Epoch 289, Loss: 0.515051\n",
            "Epoch 290, Loss: 0.514717\n",
            "Epoch 291, Loss: 0.514384\n",
            "Epoch 292, Loss: 0.514053\n",
            "Epoch 293, Loss: 0.513724\n",
            "Epoch 294, Loss: 0.513397\n",
            "Epoch 295, Loss: 0.513071\n",
            "Epoch 296, Loss: 0.512748\n",
            "Epoch 297, Loss: 0.512426\n",
            "Epoch 298, Loss: 0.512107\n",
            "Epoch 299, Loss: 0.511789\n",
            "Epoch 300, Loss: 0.511473\n",
            "Epoch 301, Loss: 0.511159\n",
            "Epoch 302, Loss: 0.510846\n",
            "Epoch 303, Loss: 0.510536\n",
            "Epoch 304, Loss: 0.510227\n",
            "Epoch 305, Loss: 0.509920\n",
            "Epoch 306, Loss: 0.509614\n",
            "Epoch 307, Loss: 0.509310\n",
            "Epoch 308, Loss: 0.509008\n",
            "Epoch 309, Loss: 0.508708\n",
            "Epoch 310, Loss: 0.508409\n",
            "Epoch 311, Loss: 0.508111\n",
            "Epoch 312, Loss: 0.507815\n",
            "Epoch 313, Loss: 0.507521\n",
            "Epoch 314, Loss: 0.507228\n",
            "Epoch 315, Loss: 0.506937\n",
            "Epoch 316, Loss: 0.506647\n",
            "Epoch 317, Loss: 0.506359\n",
            "Epoch 318, Loss: 0.506072\n",
            "Epoch 319, Loss: 0.505787\n",
            "Epoch 320, Loss: 0.505503\n",
            "Epoch 321, Loss: 0.505221\n",
            "Epoch 322, Loss: 0.504940\n",
            "Epoch 323, Loss: 0.504661\n",
            "Epoch 324, Loss: 0.504383\n",
            "Epoch 325, Loss: 0.504106\n",
            "Epoch 326, Loss: 0.503831\n",
            "Epoch 327, Loss: 0.503557\n",
            "Epoch 328, Loss: 0.503285\n",
            "Epoch 329, Loss: 0.503014\n",
            "Epoch 330, Loss: 0.502744\n",
            "Epoch 331, Loss: 0.502476\n",
            "Epoch 332, Loss: 0.502209\n",
            "Epoch 333, Loss: 0.501943\n",
            "Epoch 334, Loss: 0.501679\n",
            "Epoch 335, Loss: 0.501415\n",
            "Epoch 336, Loss: 0.501154\n",
            "Epoch 337, Loss: 0.500893\n",
            "Epoch 338, Loss: 0.500634\n",
            "Epoch 339, Loss: 0.500376\n",
            "Epoch 340, Loss: 0.500119\n",
            "Epoch 341, Loss: 0.499864\n",
            "Epoch 342, Loss: 0.499609\n",
            "Epoch 343, Loss: 0.499356\n",
            "Epoch 344, Loss: 0.499104\n",
            "Epoch 345, Loss: 0.498853\n",
            "Epoch 346, Loss: 0.498603\n",
            "Epoch 347, Loss: 0.498355\n",
            "Epoch 348, Loss: 0.498107\n",
            "Epoch 349, Loss: 0.497861\n",
            "Epoch 350, Loss: 0.497616\n",
            "Epoch 351, Loss: 0.497372\n",
            "Epoch 352, Loss: 0.497129\n",
            "Epoch 353, Loss: 0.496887\n",
            "Epoch 354, Loss: 0.496647\n",
            "Epoch 355, Loss: 0.496407\n",
            "Epoch 356, Loss: 0.496168\n",
            "Epoch 357, Loss: 0.495931\n",
            "Epoch 358, Loss: 0.495694\n",
            "Epoch 359, Loss: 0.495458\n",
            "Epoch 360, Loss: 0.495224\n",
            "Epoch 361, Loss: 0.494990\n",
            "Epoch 362, Loss: 0.494758\n",
            "Epoch 363, Loss: 0.494526\n",
            "Epoch 364, Loss: 0.494296\n",
            "Epoch 365, Loss: 0.494066\n",
            "Epoch 366, Loss: 0.493838\n",
            "Epoch 367, Loss: 0.493611\n",
            "Epoch 368, Loss: 0.493384\n",
            "Epoch 369, Loss: 0.493158\n",
            "Epoch 370, Loss: 0.492934\n",
            "Epoch 371, Loss: 0.492710\n",
            "Epoch 372, Loss: 0.492487\n",
            "Epoch 373, Loss: 0.492265\n",
            "Epoch 374, Loss: 0.492045\n",
            "Epoch 375, Loss: 0.491825\n",
            "Epoch 376, Loss: 0.491606\n",
            "Epoch 377, Loss: 0.491387\n",
            "Epoch 378, Loss: 0.491170\n",
            "Epoch 379, Loss: 0.490954\n",
            "Epoch 380, Loss: 0.490738\n",
            "Epoch 381, Loss: 0.490524\n",
            "Epoch 382, Loss: 0.490310\n",
            "Epoch 383, Loss: 0.490097\n",
            "Epoch 384, Loss: 0.489885\n",
            "Epoch 385, Loss: 0.489674\n",
            "Epoch 386, Loss: 0.489463\n",
            "Epoch 387, Loss: 0.489254\n",
            "Epoch 388, Loss: 0.489045\n",
            "Epoch 389, Loss: 0.488837\n",
            "Epoch 390, Loss: 0.488630\n",
            "Epoch 391, Loss: 0.488424\n",
            "Epoch 392, Loss: 0.488218\n",
            "Epoch 393, Loss: 0.488013\n",
            "Epoch 394, Loss: 0.487809\n",
            "Epoch 395, Loss: 0.487606\n",
            "Epoch 396, Loss: 0.487404\n",
            "Epoch 397, Loss: 0.487202\n",
            "Epoch 398, Loss: 0.487001\n",
            "Epoch 399, Loss: 0.486801\n",
            "Epoch 400, Loss: 0.486602\n",
            "Epoch 401, Loss: 0.486403\n",
            "Epoch 402, Loss: 0.486205\n",
            "Epoch 403, Loss: 0.486008\n",
            "Epoch 404, Loss: 0.485811\n",
            "Epoch 405, Loss: 0.485616\n",
            "Epoch 406, Loss: 0.485420\n",
            "Epoch 407, Loss: 0.485226\n",
            "Epoch 408, Loss: 0.485032\n",
            "Epoch 409, Loss: 0.484839\n",
            "Epoch 410, Loss: 0.484647\n",
            "Epoch 411, Loss: 0.484455\n",
            "Epoch 412, Loss: 0.484264\n",
            "Epoch 413, Loss: 0.484074\n",
            "Epoch 414, Loss: 0.483884\n",
            "Epoch 415, Loss: 0.483695\n",
            "Epoch 416, Loss: 0.483507\n",
            "Epoch 417, Loss: 0.483319\n",
            "Epoch 418, Loss: 0.483133\n",
            "Epoch 419, Loss: 0.482946\n",
            "Epoch 420, Loss: 0.482761\n",
            "Epoch 421, Loss: 0.482576\n",
            "Epoch 422, Loss: 0.482392\n",
            "Epoch 423, Loss: 0.482208\n",
            "Epoch 424, Loss: 0.482025\n",
            "Epoch 425, Loss: 0.481843\n",
            "Epoch 426, Loss: 0.481661\n",
            "Epoch 427, Loss: 0.481480\n",
            "Epoch 428, Loss: 0.481299\n",
            "Epoch 429, Loss: 0.481119\n",
            "Epoch 430, Loss: 0.480940\n",
            "Epoch 431, Loss: 0.480762\n",
            "Epoch 432, Loss: 0.480584\n",
            "Epoch 433, Loss: 0.480406\n",
            "Epoch 434, Loss: 0.480229\n",
            "Epoch 435, Loss: 0.480053\n",
            "Epoch 436, Loss: 0.479878\n",
            "Epoch 437, Loss: 0.479702\n",
            "Epoch 438, Loss: 0.479528\n",
            "Epoch 439, Loss: 0.479354\n",
            "Epoch 440, Loss: 0.479180\n",
            "Epoch 441, Loss: 0.479007\n",
            "Epoch 442, Loss: 0.478835\n",
            "Epoch 443, Loss: 0.478663\n",
            "Epoch 444, Loss: 0.478492\n",
            "Epoch 445, Loss: 0.478321\n",
            "Epoch 446, Loss: 0.478151\n",
            "Epoch 447, Loss: 0.477982\n",
            "Epoch 448, Loss: 0.477813\n",
            "Epoch 449, Loss: 0.477644\n",
            "Epoch 450, Loss: 0.477476\n",
            "Epoch 451, Loss: 0.477308\n",
            "Epoch 452, Loss: 0.477141\n",
            "Epoch 453, Loss: 0.476974\n",
            "Epoch 454, Loss: 0.476808\n",
            "Epoch 455, Loss: 0.476642\n",
            "Epoch 456, Loss: 0.476477\n",
            "Epoch 457, Loss: 0.476312\n",
            "Epoch 458, Loss: 0.476148\n",
            "Epoch 459, Loss: 0.475984\n",
            "Epoch 460, Loss: 0.475820\n",
            "Epoch 461, Loss: 0.475657\n",
            "Epoch 462, Loss: 0.475495\n",
            "Epoch 463, Loss: 0.475333\n",
            "Epoch 464, Loss: 0.475171\n",
            "Epoch 465, Loss: 0.475010\n",
            "Epoch 466, Loss: 0.474849\n",
            "Epoch 467, Loss: 0.474689\n",
            "Epoch 468, Loss: 0.474529\n",
            "Epoch 469, Loss: 0.474370\n",
            "Epoch 470, Loss: 0.474211\n",
            "Epoch 471, Loss: 0.474052\n",
            "Epoch 472, Loss: 0.473894\n",
            "Epoch 473, Loss: 0.473736\n",
            "Epoch 474, Loss: 0.473579\n",
            "Epoch 475, Loss: 0.473422\n",
            "Epoch 476, Loss: 0.473266\n",
            "Epoch 477, Loss: 0.473110\n",
            "Epoch 478, Loss: 0.472954\n",
            "Epoch 479, Loss: 0.472799\n",
            "Epoch 480, Loss: 0.472645\n",
            "Epoch 481, Loss: 0.472490\n",
            "Epoch 482, Loss: 0.472336\n",
            "Epoch 483, Loss: 0.472183\n",
            "Epoch 484, Loss: 0.472030\n",
            "Epoch 485, Loss: 0.471877\n",
            "Epoch 486, Loss: 0.471725\n",
            "Epoch 487, Loss: 0.471573\n",
            "Epoch 488, Loss: 0.471422\n",
            "Epoch 489, Loss: 0.471271\n",
            "Epoch 490, Loss: 0.471121\n",
            "Epoch 491, Loss: 0.470971\n",
            "Epoch 492, Loss: 0.470821\n",
            "Epoch 493, Loss: 0.470672\n",
            "Epoch 494, Loss: 0.470523\n",
            "Epoch 495, Loss: 0.470374\n",
            "Epoch 496, Loss: 0.470226\n",
            "Epoch 497, Loss: 0.470078\n",
            "Epoch 498, Loss: 0.469931\n",
            "Epoch 499, Loss: 0.469784\n",
            "Epoch 500, Loss: 0.469637\n",
            "Epoch 501, Loss: 0.469491\n",
            "Epoch 502, Loss: 0.469345\n",
            "Epoch 503, Loss: 0.469199\n",
            "Epoch 504, Loss: 0.469054\n",
            "Epoch 505, Loss: 0.468909\n",
            "Epoch 506, Loss: 0.468765\n",
            "Epoch 507, Loss: 0.468621\n",
            "Epoch 508, Loss: 0.468477\n",
            "Epoch 509, Loss: 0.468333\n",
            "Epoch 510, Loss: 0.468190\n",
            "Epoch 511, Loss: 0.468048\n",
            "Epoch 512, Loss: 0.467906\n",
            "Epoch 513, Loss: 0.467764\n",
            "Epoch 514, Loss: 0.467622\n",
            "Epoch 515, Loss: 0.467481\n",
            "Epoch 516, Loss: 0.467340\n",
            "Epoch 517, Loss: 0.467199\n",
            "Epoch 518, Loss: 0.467059\n",
            "Epoch 519, Loss: 0.466919\n",
            "Epoch 520, Loss: 0.466779\n",
            "Epoch 521, Loss: 0.466640\n",
            "Epoch 522, Loss: 0.466501\n",
            "Epoch 523, Loss: 0.466362\n",
            "Epoch 524, Loss: 0.466224\n",
            "Epoch 525, Loss: 0.466086\n",
            "Epoch 526, Loss: 0.465948\n",
            "Epoch 527, Loss: 0.465811\n",
            "Epoch 528, Loss: 0.465674\n",
            "Epoch 529, Loss: 0.465537\n",
            "Epoch 530, Loss: 0.465400\n",
            "Epoch 531, Loss: 0.465264\n",
            "Epoch 532, Loss: 0.465128\n",
            "Epoch 533, Loss: 0.464992\n",
            "Epoch 534, Loss: 0.464857\n",
            "Epoch 535, Loss: 0.464722\n",
            "Epoch 536, Loss: 0.464587\n",
            "Epoch 537, Loss: 0.464452\n",
            "Epoch 538, Loss: 0.464318\n",
            "Epoch 539, Loss: 0.464184\n",
            "Epoch 540, Loss: 0.464050\n",
            "Epoch 541, Loss: 0.463917\n",
            "Epoch 542, Loss: 0.463784\n",
            "Epoch 543, Loss: 0.463651\n",
            "Epoch 544, Loss: 0.463518\n",
            "Epoch 545, Loss: 0.463386\n",
            "Epoch 546, Loss: 0.463254\n",
            "Epoch 547, Loss: 0.463123\n",
            "Epoch 548, Loss: 0.462991\n",
            "Epoch 549, Loss: 0.462860\n",
            "Epoch 550, Loss: 0.462729\n",
            "Epoch 551, Loss: 0.462598\n",
            "Epoch 552, Loss: 0.462468\n",
            "Epoch 553, Loss: 0.462337\n",
            "Epoch 554, Loss: 0.462207\n",
            "Epoch 555, Loss: 0.462078\n",
            "Epoch 556, Loss: 0.461948\n",
            "Epoch 557, Loss: 0.461819\n",
            "Epoch 558, Loss: 0.461690\n",
            "Epoch 559, Loss: 0.461561\n",
            "Epoch 560, Loss: 0.461433\n",
            "Epoch 561, Loss: 0.461304\n",
            "Epoch 562, Loss: 0.461176\n",
            "Epoch 563, Loss: 0.461048\n",
            "Epoch 564, Loss: 0.460921\n",
            "Epoch 565, Loss: 0.460793\n",
            "Epoch 566, Loss: 0.460666\n",
            "Epoch 567, Loss: 0.460539\n",
            "Epoch 568, Loss: 0.460412\n",
            "Epoch 569, Loss: 0.460286\n",
            "Epoch 570, Loss: 0.460160\n",
            "Epoch 571, Loss: 0.460034\n",
            "Epoch 572, Loss: 0.459908\n",
            "Epoch 573, Loss: 0.459782\n",
            "Epoch 574, Loss: 0.459657\n",
            "Epoch 575, Loss: 0.459532\n",
            "Epoch 576, Loss: 0.459407\n",
            "Epoch 577, Loss: 0.459282\n",
            "Epoch 578, Loss: 0.459158\n",
            "Epoch 579, Loss: 0.459033\n",
            "Epoch 580, Loss: 0.458909\n",
            "Epoch 581, Loss: 0.458785\n",
            "Epoch 582, Loss: 0.458661\n",
            "Epoch 583, Loss: 0.458538\n",
            "Epoch 584, Loss: 0.458414\n",
            "Epoch 585, Loss: 0.458291\n",
            "Epoch 586, Loss: 0.458168\n",
            "Epoch 587, Loss: 0.458046\n",
            "Epoch 588, Loss: 0.457923\n",
            "Epoch 589, Loss: 0.457801\n",
            "Epoch 590, Loss: 0.457678\n",
            "Epoch 591, Loss: 0.457556\n",
            "Epoch 592, Loss: 0.457434\n",
            "Epoch 593, Loss: 0.457313\n",
            "Epoch 594, Loss: 0.457191\n",
            "Epoch 595, Loss: 0.457070\n",
            "Epoch 596, Loss: 0.456949\n",
            "Epoch 597, Loss: 0.456829\n",
            "Epoch 598, Loss: 0.456708\n",
            "Epoch 599, Loss: 0.456588\n",
            "Epoch 600, Loss: 0.456467\n",
            "Epoch 601, Loss: 0.456347\n",
            "Epoch 602, Loss: 0.456227\n",
            "Epoch 603, Loss: 0.456108\n",
            "Epoch 604, Loss: 0.455988\n",
            "Epoch 605, Loss: 0.455869\n",
            "Epoch 606, Loss: 0.455750\n",
            "Epoch 607, Loss: 0.455631\n",
            "Epoch 608, Loss: 0.455512\n",
            "Epoch 609, Loss: 0.455394\n",
            "Epoch 610, Loss: 0.455275\n",
            "Epoch 611, Loss: 0.455157\n",
            "Epoch 612, Loss: 0.455039\n",
            "Epoch 613, Loss: 0.454921\n",
            "Epoch 614, Loss: 0.454803\n",
            "Epoch 615, Loss: 0.454685\n",
            "Epoch 616, Loss: 0.454568\n",
            "Epoch 617, Loss: 0.454451\n",
            "Epoch 618, Loss: 0.454334\n",
            "Epoch 619, Loss: 0.454217\n",
            "Epoch 620, Loss: 0.454100\n",
            "Epoch 621, Loss: 0.453983\n",
            "Epoch 622, Loss: 0.453867\n",
            "Epoch 623, Loss: 0.453750\n",
            "Epoch 624, Loss: 0.453634\n",
            "Epoch 625, Loss: 0.453518\n",
            "Epoch 626, Loss: 0.453402\n",
            "Epoch 627, Loss: 0.453287\n",
            "Epoch 628, Loss: 0.453171\n",
            "Epoch 629, Loss: 0.453056\n",
            "Epoch 630, Loss: 0.452941\n",
            "Epoch 631, Loss: 0.452826\n",
            "Epoch 632, Loss: 0.452711\n",
            "Epoch 633, Loss: 0.452596\n",
            "Epoch 634, Loss: 0.452481\n",
            "Epoch 635, Loss: 0.452367\n",
            "Epoch 636, Loss: 0.452252\n",
            "Epoch 637, Loss: 0.452138\n",
            "Epoch 638, Loss: 0.452024\n",
            "Epoch 639, Loss: 0.451910\n",
            "Epoch 640, Loss: 0.451796\n",
            "Epoch 641, Loss: 0.451683\n",
            "Epoch 642, Loss: 0.451569\n",
            "Epoch 643, Loss: 0.451456\n",
            "Epoch 644, Loss: 0.451343\n",
            "Epoch 645, Loss: 0.451230\n",
            "Epoch 646, Loss: 0.451117\n",
            "Epoch 647, Loss: 0.451004\n",
            "Epoch 648, Loss: 0.450891\n",
            "Epoch 649, Loss: 0.450778\n",
            "Epoch 650, Loss: 0.450666\n",
            "Epoch 651, Loss: 0.450553\n",
            "Epoch 652, Loss: 0.450441\n",
            "Epoch 653, Loss: 0.450329\n",
            "Epoch 654, Loss: 0.450217\n",
            "Epoch 655, Loss: 0.450105\n",
            "Epoch 656, Loss: 0.449993\n",
            "Epoch 657, Loss: 0.449882\n",
            "Epoch 658, Loss: 0.449770\n",
            "Epoch 659, Loss: 0.449659\n",
            "Epoch 660, Loss: 0.449548\n",
            "Epoch 661, Loss: 0.449437\n",
            "Epoch 662, Loss: 0.449326\n",
            "Epoch 663, Loss: 0.449216\n",
            "Epoch 664, Loss: 0.449105\n",
            "Epoch 665, Loss: 0.448995\n",
            "Epoch 666, Loss: 0.448885\n",
            "Epoch 667, Loss: 0.448775\n",
            "Epoch 668, Loss: 0.448665\n",
            "Epoch 669, Loss: 0.448555\n",
            "Epoch 670, Loss: 0.448445\n",
            "Epoch 671, Loss: 0.448336\n",
            "Epoch 672, Loss: 0.448226\n",
            "Epoch 673, Loss: 0.448117\n",
            "Epoch 674, Loss: 0.448008\n",
            "Epoch 675, Loss: 0.447899\n",
            "Epoch 676, Loss: 0.447790\n",
            "Epoch 677, Loss: 0.447681\n",
            "Epoch 678, Loss: 0.447572\n",
            "Epoch 679, Loss: 0.447464\n",
            "Epoch 680, Loss: 0.447355\n",
            "Epoch 681, Loss: 0.447247\n",
            "Epoch 682, Loss: 0.447139\n",
            "Epoch 683, Loss: 0.447031\n",
            "Epoch 684, Loss: 0.446923\n",
            "Epoch 685, Loss: 0.446816\n",
            "Epoch 686, Loss: 0.446708\n",
            "Epoch 687, Loss: 0.446600\n",
            "Epoch 688, Loss: 0.446493\n",
            "Epoch 689, Loss: 0.446386\n",
            "Epoch 690, Loss: 0.446278\n",
            "Epoch 691, Loss: 0.446171\n",
            "Epoch 692, Loss: 0.446065\n",
            "Epoch 693, Loss: 0.445958\n",
            "Epoch 694, Loss: 0.445851\n",
            "Epoch 695, Loss: 0.445745\n",
            "Epoch 696, Loss: 0.445638\n",
            "Epoch 697, Loss: 0.445532\n",
            "Epoch 698, Loss: 0.445426\n",
            "Epoch 699, Loss: 0.445319\n",
            "Epoch 700, Loss: 0.445213\n",
            "Epoch 701, Loss: 0.445107\n",
            "Epoch 702, Loss: 0.445002\n",
            "Epoch 703, Loss: 0.444896\n",
            "Epoch 704, Loss: 0.444790\n",
            "Epoch 705, Loss: 0.444685\n",
            "Epoch 706, Loss: 0.444579\n",
            "Epoch 707, Loss: 0.444474\n",
            "Epoch 708, Loss: 0.444369\n",
            "Epoch 709, Loss: 0.444264\n",
            "Epoch 710, Loss: 0.444158\n",
            "Epoch 711, Loss: 0.444053\n",
            "Epoch 712, Loss: 0.443949\n",
            "Epoch 713, Loss: 0.443844\n",
            "Epoch 714, Loss: 0.443739\n",
            "Epoch 715, Loss: 0.443635\n",
            "Epoch 716, Loss: 0.443530\n",
            "Epoch 717, Loss: 0.443426\n",
            "Epoch 718, Loss: 0.443322\n",
            "Epoch 719, Loss: 0.443218\n",
            "Epoch 720, Loss: 0.443114\n",
            "Epoch 721, Loss: 0.443010\n",
            "Epoch 722, Loss: 0.442906\n",
            "Epoch 723, Loss: 0.442803\n",
            "Epoch 724, Loss: 0.442699\n",
            "Epoch 725, Loss: 0.442596\n",
            "Epoch 726, Loss: 0.442492\n",
            "Epoch 727, Loss: 0.442389\n",
            "Epoch 728, Loss: 0.442286\n",
            "Epoch 729, Loss: 0.442183\n",
            "Epoch 730, Loss: 0.442080\n",
            "Epoch 731, Loss: 0.441977\n",
            "Epoch 732, Loss: 0.441874\n",
            "Epoch 733, Loss: 0.441771\n",
            "Epoch 734, Loss: 0.441669\n",
            "Epoch 735, Loss: 0.441566\n",
            "Epoch 736, Loss: 0.441463\n",
            "Epoch 737, Loss: 0.441361\n",
            "Epoch 738, Loss: 0.441258\n",
            "Epoch 739, Loss: 0.441156\n",
            "Epoch 740, Loss: 0.441054\n",
            "Epoch 741, Loss: 0.440951\n",
            "Epoch 742, Loss: 0.440849\n",
            "Epoch 743, Loss: 0.440747\n",
            "Epoch 744, Loss: 0.440645\n",
            "Epoch 745, Loss: 0.440543\n",
            "Epoch 746, Loss: 0.440442\n",
            "Epoch 747, Loss: 0.440340\n",
            "Epoch 748, Loss: 0.440239\n",
            "Epoch 749, Loss: 0.440137\n",
            "Epoch 750, Loss: 0.440036\n",
            "Epoch 751, Loss: 0.439935\n",
            "Epoch 752, Loss: 0.439833\n",
            "Epoch 753, Loss: 0.439732\n",
            "Epoch 754, Loss: 0.439631\n",
            "Epoch 755, Loss: 0.439530\n",
            "Epoch 756, Loss: 0.439429\n",
            "Epoch 757, Loss: 0.439329\n",
            "Epoch 758, Loss: 0.439228\n",
            "Epoch 759, Loss: 0.439127\n",
            "Epoch 760, Loss: 0.439027\n",
            "Epoch 761, Loss: 0.438926\n",
            "Epoch 762, Loss: 0.438826\n",
            "Epoch 763, Loss: 0.438726\n",
            "Epoch 764, Loss: 0.438625\n",
            "Epoch 765, Loss: 0.438525\n",
            "Epoch 766, Loss: 0.438425\n",
            "Epoch 767, Loss: 0.438325\n",
            "Epoch 768, Loss: 0.438225\n",
            "Epoch 769, Loss: 0.438125\n",
            "Epoch 770, Loss: 0.438025\n",
            "Epoch 771, Loss: 0.437926\n",
            "Epoch 772, Loss: 0.437826\n",
            "Epoch 773, Loss: 0.437726\n",
            "Epoch 774, Loss: 0.437627\n",
            "Epoch 775, Loss: 0.437527\n",
            "Epoch 776, Loss: 0.437428\n",
            "Epoch 777, Loss: 0.437328\n",
            "Epoch 778, Loss: 0.437229\n",
            "Epoch 779, Loss: 0.437130\n",
            "Epoch 780, Loss: 0.437030\n",
            "Epoch 781, Loss: 0.436931\n",
            "Epoch 782, Loss: 0.436832\n",
            "Epoch 783, Loss: 0.436733\n",
            "Epoch 784, Loss: 0.436634\n",
            "Epoch 785, Loss: 0.436535\n",
            "Epoch 786, Loss: 0.436437\n",
            "Epoch 787, Loss: 0.436338\n",
            "Epoch 788, Loss: 0.436239\n",
            "Epoch 789, Loss: 0.436140\n",
            "Epoch 790, Loss: 0.436042\n",
            "Epoch 791, Loss: 0.435943\n",
            "Epoch 792, Loss: 0.435845\n",
            "Epoch 793, Loss: 0.435746\n",
            "Epoch 794, Loss: 0.435648\n",
            "Epoch 795, Loss: 0.435549\n",
            "Epoch 796, Loss: 0.435451\n",
            "Epoch 797, Loss: 0.435353\n",
            "Epoch 798, Loss: 0.435255\n",
            "Epoch 799, Loss: 0.435157\n",
            "Epoch 800, Loss: 0.435059\n",
            "Epoch 801, Loss: 0.434960\n",
            "Epoch 802, Loss: 0.434862\n",
            "Epoch 803, Loss: 0.434764\n",
            "Epoch 804, Loss: 0.434667\n",
            "Epoch 805, Loss: 0.434569\n",
            "Epoch 806, Loss: 0.434471\n",
            "Epoch 807, Loss: 0.434373\n",
            "Epoch 808, Loss: 0.434275\n",
            "Epoch 809, Loss: 0.434178\n",
            "Epoch 810, Loss: 0.434080\n",
            "Epoch 811, Loss: 0.433983\n",
            "Epoch 812, Loss: 0.433885\n",
            "Epoch 813, Loss: 0.433788\n",
            "Epoch 814, Loss: 0.433691\n",
            "Epoch 815, Loss: 0.433593\n",
            "Epoch 816, Loss: 0.433496\n",
            "Epoch 817, Loss: 0.433399\n",
            "Epoch 818, Loss: 0.433302\n",
            "Epoch 819, Loss: 0.433205\n",
            "Epoch 820, Loss: 0.433108\n",
            "Epoch 821, Loss: 0.433012\n",
            "Epoch 822, Loss: 0.432915\n",
            "Epoch 823, Loss: 0.432818\n",
            "Epoch 824, Loss: 0.432721\n",
            "Epoch 825, Loss: 0.432625\n",
            "Epoch 826, Loss: 0.432528\n",
            "Epoch 827, Loss: 0.432432\n",
            "Epoch 828, Loss: 0.432335\n",
            "Epoch 829, Loss: 0.432239\n",
            "Epoch 830, Loss: 0.432143\n",
            "Epoch 831, Loss: 0.432047\n",
            "Epoch 832, Loss: 0.431951\n",
            "Epoch 833, Loss: 0.431855\n",
            "Epoch 834, Loss: 0.431759\n",
            "Epoch 835, Loss: 0.431663\n",
            "Epoch 836, Loss: 0.431567\n",
            "Epoch 837, Loss: 0.431471\n",
            "Epoch 838, Loss: 0.431375\n",
            "Epoch 839, Loss: 0.431279\n",
            "Epoch 840, Loss: 0.431183\n",
            "Epoch 841, Loss: 0.431088\n",
            "Epoch 842, Loss: 0.430992\n",
            "Epoch 843, Loss: 0.430897\n",
            "Epoch 844, Loss: 0.430801\n",
            "Epoch 845, Loss: 0.430706\n",
            "Epoch 846, Loss: 0.430610\n",
            "Epoch 847, Loss: 0.430515\n",
            "Epoch 848, Loss: 0.430420\n",
            "Epoch 849, Loss: 0.430324\n",
            "Epoch 850, Loss: 0.430229\n",
            "Epoch 851, Loss: 0.430134\n",
            "Epoch 852, Loss: 0.430039\n",
            "Epoch 853, Loss: 0.429944\n",
            "Epoch 854, Loss: 0.429849\n",
            "Epoch 855, Loss: 0.429754\n",
            "Epoch 856, Loss: 0.429659\n",
            "Epoch 857, Loss: 0.429564\n",
            "Epoch 858, Loss: 0.429469\n",
            "Epoch 859, Loss: 0.429375\n",
            "Epoch 860, Loss: 0.429280\n",
            "Epoch 861, Loss: 0.429185\n",
            "Epoch 862, Loss: 0.429091\n",
            "Epoch 863, Loss: 0.428996\n",
            "Epoch 864, Loss: 0.428902\n",
            "Epoch 865, Loss: 0.428807\n",
            "Epoch 866, Loss: 0.428713\n",
            "Epoch 867, Loss: 0.428618\n",
            "Epoch 868, Loss: 0.428524\n",
            "Epoch 869, Loss: 0.428430\n",
            "Epoch 870, Loss: 0.428336\n",
            "Epoch 871, Loss: 0.428241\n",
            "Epoch 872, Loss: 0.428147\n",
            "Epoch 873, Loss: 0.428053\n",
            "Epoch 874, Loss: 0.427959\n",
            "Epoch 875, Loss: 0.427865\n",
            "Epoch 876, Loss: 0.427771\n",
            "Epoch 877, Loss: 0.427677\n",
            "Epoch 878, Loss: 0.427583\n",
            "Epoch 879, Loss: 0.427490\n",
            "Epoch 880, Loss: 0.427396\n",
            "Epoch 881, Loss: 0.427302\n",
            "Epoch 882, Loss: 0.427208\n",
            "Epoch 883, Loss: 0.427115\n",
            "Epoch 884, Loss: 0.427021\n",
            "Epoch 885, Loss: 0.426927\n",
            "Epoch 886, Loss: 0.426834\n",
            "Epoch 887, Loss: 0.426740\n",
            "Epoch 888, Loss: 0.426647\n",
            "Epoch 889, Loss: 0.426554\n",
            "Epoch 890, Loss: 0.426460\n",
            "Epoch 891, Loss: 0.426367\n",
            "Epoch 892, Loss: 0.426274\n",
            "Epoch 893, Loss: 0.426181\n",
            "Epoch 894, Loss: 0.426087\n",
            "Epoch 895, Loss: 0.425994\n",
            "Epoch 896, Loss: 0.425901\n",
            "Epoch 897, Loss: 0.425808\n",
            "Epoch 898, Loss: 0.425715\n",
            "Epoch 899, Loss: 0.425622\n",
            "Epoch 900, Loss: 0.425529\n",
            "Epoch 901, Loss: 0.425436\n",
            "Epoch 902, Loss: 0.425343\n",
            "Epoch 903, Loss: 0.425250\n",
            "Epoch 904, Loss: 0.425157\n",
            "Epoch 905, Loss: 0.425065\n",
            "Epoch 906, Loss: 0.424972\n",
            "Epoch 907, Loss: 0.424879\n",
            "Epoch 908, Loss: 0.424786\n",
            "Epoch 909, Loss: 0.424694\n",
            "Epoch 910, Loss: 0.424601\n",
            "Epoch 911, Loss: 0.424509\n",
            "Epoch 912, Loss: 0.424416\n",
            "Epoch 913, Loss: 0.424324\n",
            "Epoch 914, Loss: 0.424232\n",
            "Epoch 915, Loss: 0.424139\n",
            "Epoch 916, Loss: 0.424047\n",
            "Epoch 917, Loss: 0.423955\n",
            "Epoch 918, Loss: 0.423863\n",
            "Epoch 919, Loss: 0.423770\n",
            "Epoch 920, Loss: 0.423678\n",
            "Epoch 921, Loss: 0.423586\n",
            "Epoch 922, Loss: 0.423494\n",
            "Epoch 923, Loss: 0.423402\n",
            "Epoch 924, Loss: 0.423310\n",
            "Epoch 925, Loss: 0.423218\n",
            "Epoch 926, Loss: 0.423126\n",
            "Epoch 927, Loss: 0.423034\n",
            "Epoch 928, Loss: 0.422942\n",
            "Epoch 929, Loss: 0.422851\n",
            "Epoch 930, Loss: 0.422759\n",
            "Epoch 931, Loss: 0.422667\n",
            "Epoch 932, Loss: 0.422575\n",
            "Epoch 933, Loss: 0.422484\n",
            "Epoch 934, Loss: 0.422392\n",
            "Epoch 935, Loss: 0.422300\n",
            "Epoch 936, Loss: 0.422209\n",
            "Epoch 937, Loss: 0.422117\n",
            "Epoch 938, Loss: 0.422026\n",
            "Epoch 939, Loss: 0.421934\n",
            "Epoch 940, Loss: 0.421843\n",
            "Epoch 941, Loss: 0.421752\n",
            "Epoch 942, Loss: 0.421660\n",
            "Epoch 943, Loss: 0.421569\n",
            "Epoch 944, Loss: 0.421478\n",
            "Epoch 945, Loss: 0.421387\n",
            "Epoch 946, Loss: 0.421295\n",
            "Epoch 947, Loss: 0.421204\n",
            "Epoch 948, Loss: 0.421113\n",
            "Epoch 949, Loss: 0.421022\n",
            "Epoch 950, Loss: 0.420931\n",
            "Epoch 951, Loss: 0.420840\n",
            "Epoch 952, Loss: 0.420749\n",
            "Epoch 953, Loss: 0.420658\n",
            "Epoch 954, Loss: 0.420568\n",
            "Epoch 955, Loss: 0.420477\n",
            "Epoch 956, Loss: 0.420386\n",
            "Epoch 957, Loss: 0.420295\n",
            "Epoch 958, Loss: 0.420204\n",
            "Epoch 959, Loss: 0.420114\n",
            "Epoch 960, Loss: 0.420023\n",
            "Epoch 961, Loss: 0.419933\n",
            "Epoch 962, Loss: 0.419842\n",
            "Epoch 963, Loss: 0.419751\n",
            "Epoch 964, Loss: 0.419661\n",
            "Epoch 965, Loss: 0.419570\n",
            "Epoch 966, Loss: 0.419480\n",
            "Epoch 967, Loss: 0.419389\n",
            "Epoch 968, Loss: 0.419299\n",
            "Epoch 969, Loss: 0.419209\n",
            "Epoch 970, Loss: 0.419118\n",
            "Epoch 971, Loss: 0.419028\n",
            "Epoch 972, Loss: 0.418938\n",
            "Epoch 973, Loss: 0.418847\n",
            "Epoch 974, Loss: 0.418757\n",
            "Epoch 975, Loss: 0.418667\n",
            "Epoch 976, Loss: 0.418577\n",
            "Epoch 977, Loss: 0.418487\n",
            "Epoch 978, Loss: 0.418396\n",
            "Epoch 979, Loss: 0.418306\n",
            "Epoch 980, Loss: 0.418216\n",
            "Epoch 981, Loss: 0.418126\n",
            "Epoch 982, Loss: 0.418036\n",
            "Epoch 983, Loss: 0.417946\n",
            "Epoch 984, Loss: 0.417856\n",
            "Epoch 985, Loss: 0.417766\n",
            "Epoch 986, Loss: 0.417676\n",
            "Epoch 987, Loss: 0.417586\n",
            "Epoch 988, Loss: 0.417496\n",
            "Epoch 989, Loss: 0.417406\n",
            "Epoch 990, Loss: 0.417317\n",
            "Epoch 991, Loss: 0.417227\n",
            "Epoch 992, Loss: 0.417137\n",
            "Epoch 993, Loss: 0.417047\n",
            "Epoch 994, Loss: 0.416958\n",
            "Epoch 995, Loss: 0.416868\n",
            "Epoch 996, Loss: 0.416779\n",
            "Epoch 997, Loss: 0.416689\n",
            "Epoch 998, Loss: 0.416599\n",
            "Epoch 999, Loss: 0.416510\n",
            "Epoch 1000, Loss: 0.416420\n",
            "Test Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bsm3ZCWBkI0g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}